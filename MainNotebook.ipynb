{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MainNotebook",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14cPYlV288WKyy41AFK7fvn-W5k5BIiJP",
      "authorship_tag": "ABX9TyP7mt27DrPLHdkRVn26KJ/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CianOSull/AutoML_With_SA_FYP_2021/blob/Preprocessing/MainNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_zrt8fk5AJt"
      },
      "source": [
        "# Generic Notebook for running all the libraries\n",
        "\n",
        "How this notebook works is that it contains the code\n",
        "for loading and cleaning the dataset.\n",
        "\n",
        "Then there is multiple branches created on the\n",
        "Github that include the code for running each library.\n",
        "\n",
        "E.g. MLBox branch has the code for running MLBox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJSxy1VR-mAM"
      },
      "source": [
        "# CURRENT BRANCH: [INSERT BRANCH NAME HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxc3argyH4PH"
      },
      "source": [
        "# Install Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wFpyMaK6tMs"
      },
      "source": [
        "# Insert any install comamnds in this cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TNIZJ6c5hcF"
      },
      "source": [
        "# Preprocessing Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njEYGcZQ42t-"
      },
      "source": [
        "# Import the necessary modules for cleaning\n",
        "import math\n",
        "import time \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93NNPX-A5srS",
        "outputId": "14935d5c-d77e-45b9-ffff-2e5e9ee040b4"
      },
      "source": [
        "# Download the necessary parts for the NLTK module\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew8tq1lj6v1L"
      },
      "source": [
        "# Create Model Section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Za96N47nEE"
      },
      "source": [
        "# SKLearn Inbuilt tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER1WCdTB7qd7"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# A dense numpy array of the sparse matrix outputted can be made by using:\n",
        "# X_train.toarray()\n",
        "def create_data(num, process):  \n",
        "  imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\n",
        "\n",
        "  # the next step is to randomize the rows of the data\n",
        "  imdb_df = imdb_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\n",
        "\n",
        "  features = imdb_df.review.values[0:num]\n",
        "  labels = imdb_df.sentiment.values[0:num]\n",
        "\n",
        "  no_samples = 0.8\n",
        "\n",
        "  # This gets the percentage of indexes from feature vector and uses those for training\n",
        "  train_txt = features[0:int(no_samples*len(features))]\n",
        "  y_train = labels[0:int(no_samples*len(labels))]\n",
        "\n",
        "  # Go from the index that was used for training to the final\n",
        "  test_txt = features[int(no_samples*len(features)):len(features)]\n",
        "  y_test = labels[int(no_samples*len(labels)):len(labels)]\n",
        "\n",
        "  # This cell has the sk learn functions\n",
        "  # Allows you to choose which function you want to use\n",
        "  if process == 0:\n",
        "    ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    ctv.fit(features)\n",
        "\n",
        "    X_train_sparse = ctv.transform(train_txt)\n",
        "    X_test_sparse = ctv.transform(test_txt)\n",
        "\n",
        "  else:\n",
        "    tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    tfv.fit(features)\n",
        "\n",
        "    X_train_sparse = tfv.transform(train_txt)\n",
        "    X_test_sparse = tfv.transform(test_txt)\n",
        "\n",
        "  # Converting to numpy arrays for more generic format\n",
        "  X_train = X_train_sparse.toarray()\n",
        "  X_test = X_test_sparse.toarray()\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM2a_rmZog5T"
      },
      "source": [
        "# Create train test split\n",
        "X_train, y_train, X_test, y_test = create_data(500, 1)\n",
        "print(type(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fXai0gzn7CR"
      },
      "source": [
        "# Run the automl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZrWS-Kzn-Sh"
      },
      "source": [
        "# Insert AutoML code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwuEtoLxNPVZ"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUUN-JOsoBgf"
      },
      "source": [
        "# Create the variable y_pred that will contain the predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROHODdg4xupC",
        "outputId": "47ad624a-b006-4a0e-f66a-0fa1857ca01d"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.75      0.74        52\n",
            "           1       0.72      0.69      0.70        48\n",
            "\n",
            "    accuracy                           0.72       100\n",
            "   macro avg       0.72      0.72      0.72       100\n",
            "weighted avg       0.72      0.72      0.72       100\n",
            "\n",
            "[[39 13]\n",
            " [15 33]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMmhj0AtzdrL"
      },
      "source": [
        "# Export The Models/Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df0s1DH6oIfk"
      },
      "source": [
        "# Insert any code to export the models/pipelines created\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yOniZVL12fl"
      },
      "source": [
        "# Test Dataset\n",
        "\n",
        "This area is for running the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TciiscIY75Cn"
      },
      "source": [
        "# A dense numpy array of the sparse matrix outputted can be made by using:\n",
        "# X_train.toarray()\n",
        "def create_test_data(process):  \n",
        "  test_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/NPS_TestContent.csv\", encoding='utf8', engine='c')\n",
        "  \n",
        "  # 2 null rows were found so drop them\n",
        "  print(test_df.isnull().sum().sum())\n",
        "  test_df = test_df.dropna()\n",
        "  print(test_df.isnull().sum().sum())\n",
        "\n",
        "  # the next step is to randomize the rows of the data\n",
        "  test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  test_df['sentiment'] = test_df['sentiment'].map({'Negative' : 0, 'Positive' : 1})\n",
        "\n",
        "  features = test_df.test.values\n",
        "  labels = test_df.sentiment.values\n",
        "\n",
        "  no_samples = 0.8\n",
        "\n",
        "  # This cell has the sk learn functions\n",
        "  # Allows you to choose which function you want to use\n",
        "  if process == 0:\n",
        "    ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    ctv.fit(features)\n",
        "\n",
        "    features_sparse = ctv.transform(features)\n",
        "\n",
        "  else:\n",
        "    tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    tfv.fit(features)\n",
        "\n",
        "    features_sparse = tfv.transform(features)\n",
        "\n",
        "  # Converting to numpy arrays for more generic format\n",
        "  features_t = features_sparse.toarray()\n",
        "\n",
        "  return features_t, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN4lhwEA8l0-"
      },
      "source": [
        "features, labels = create_test_data(1)\n",
        "\n",
        "# Multiple column format\n",
        "tfidf_df = pd.DataFrame(features)\n",
        "tfidf_df['labels'] = labels\n",
        "\n",
        "print(tfidf_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhHW-JFuA605"
      },
      "source": [
        "# Exported Pipeline/Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qMoO2Lk47RY"
      },
      "source": [
        "# Create train test split\n",
        "X_train, y_train, X_test, y_test = create_data(500, 1)\n",
        "print(type(X_train))\n",
        "\n",
        "# Insert the calling of exported pipelines/models here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdCoplWmAjg4"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(testing_target, results))\n",
        "print(metrics.confusion_matrix(testing_target, results))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}