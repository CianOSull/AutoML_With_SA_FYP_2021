{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AutoKeras MainNotebook",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BCwiGglBmC7bxxANekjGqMCvEj2Fc4jP",
      "authorship_tag": "ABX9TyO6WAVu2MjmTEXrvdvOGnbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CianOSull/AutoML_With_SA_FYP_2021/blob/AutoKeras/MainNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_zrt8fk5AJt"
      },
      "source": [
        "# Generic Notebook for running all the libraries\r\n",
        "\r\n",
        "How this notebook works is that it contains the code\r\n",
        "for loading and cleaning the dataset.\r\n",
        "\r\n",
        "Then there is multiple branches created on the\r\n",
        "Github that include the code for running each library.\r\n",
        "\r\n",
        "E.g. MLBox branch has the code for running MLBox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFljFvk_9b_e"
      },
      "source": [
        "# CURRENT BRANCH = AUTOKERAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew8tq1lj6v1L"
      },
      "source": [
        "# Install the necessary library\r\n",
        "Run the install code in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wFpyMaK6tMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f095c1d6-962e-4dc2-b796-7eef07b0ffd8"
      },
      "source": [
        "# Insert any install comamnds in this cell\r\n",
        "!pip3 install autokeras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autokeras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/12/cf698586ccc8245f08d1843dcafb65b064a2e9e2923b889dc58e1019f099/autokeras-1.0.12-py3-none-any.whl (164kB)\n",
            "\r\u001b[K     |██                              | 10kB 13.7MB/s eta 0:00:01\r\u001b[K     |████                            | 20kB 13.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 102kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 112kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 122kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 133kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 143kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 153kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autokeras) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.1.5)\n",
            "Collecting keras-tuner>=1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from autokeras) (20.9)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from autokeras) (2.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (0.8.8)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->autokeras) (2.4.7)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.12.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.4.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (1.24.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.4.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (53.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.27.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (4.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.4.8)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=c19df5fa533f5d50ddec96bc4dd1ea2aa50fdd7cd2d0ffab0329ff4f1aed6822\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=bb357ad67f50d967fcb3ecf100228756099f3ba6719a1049ab84732b4b5fd75b\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner, autokeras\n",
            "Successfully installed autokeras-1.0.12 colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TNIZJ6c5hcF"
      },
      "source": [
        "# Preprocessing Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njEYGcZQ42t-"
      },
      "source": [
        "# Import the necessary modules for cleaning\r\n",
        "import math\r\n",
        "import time \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sPOCEGS5Xgn"
      },
      "source": [
        "# Create the set of stopwords for cleaning text\r\n",
        "stopwords = set(w.rstrip() for w in open('/content/drive/MyDrive/CIT/FYP/ImplementationFiles/stopwords.txt'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93NNPX-A5srS",
        "outputId": "70ee6ea4-e52c-4d26-d879-a588c1dc66ba"
      },
      "source": [
        "# Download the necessary parts for the NLTK module\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ptneBc5tAN"
      },
      "source": [
        "# This funciton handles celaning text\r\n",
        "def clean_text(text):\r\n",
        "    # Create the lemmatizer\r\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "    \r\n",
        "    # Get rid of non alpha characters except \"'\" as it is needed for the lemment\r\n",
        "    text = \"\".join(c for c in text if c.isalnum() or c == \" \" or \"'\")\r\n",
        "    \r\n",
        "    # Get rid of capitals\r\n",
        "    text = text.lower()\r\n",
        "    \r\n",
        "    # Tokenize the words    \r\n",
        "    # Create tokens of each word\r\n",
        "    token_text = word_tokenize(text)\r\n",
        "    \r\n",
        "    # Get rid of any piece of text that isn't over 2 characters\r\n",
        "    token_text = [t for t in token_text if len(t) > 2] \r\n",
        "    \r\n",
        "    # Put words in base form by doing lemmatization\r\n",
        "    token_text = [wordnet_lemmatizer.lemmatize(t) for t in token_text]\r\n",
        "\r\n",
        "    # Remove stopwords\r\n",
        "    token_text = [t for t in token_text if t not in stopwords]\r\n",
        "    \r\n",
        "    # Return the tokens\r\n",
        "    return token_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu5dFDM75tjR"
      },
      "source": [
        "# This function will get the term frequencies for word in the review\r\n",
        "# TF = Term I frequency in document/total words in document\r\n",
        "def calc_tf(term_count, review_corpus):\r\n",
        "    # A dictionary of all the term frequencies found\r\n",
        "    tf_freq = dict.fromkeys(term_count.keys(), 0)   \r\n",
        "    \r\n",
        "    # Review corpus is a tokenized list so the total words iteh length\r\n",
        "    total_words = len(review_corpus)\r\n",
        "    \r\n",
        "    # Calculate the term frequency for each word\r\n",
        "    for word, count in term_count.items():\r\n",
        "        tf_freq[word] = count/total_words\r\n",
        "        \r\n",
        "    return tf_freq"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i64Bq0rH5uN0"
      },
      "source": [
        "# This calcualtes the idf\r\n",
        "# IDF = log(2)*(Total number of Documents/documents frequency or documents with term)\r\n",
        "def calc_idf(unique_terms, list_doc_terms):   \r\n",
        "    # A dicitonary of all the inverse document frequencies\r\n",
        "    idf = dict.fromkeys(unique_terms, 0)\r\n",
        "    \r\n",
        "    # Basically list_doc_terms has all the documents with the term count for each word\r\n",
        "    # You go through each document count the terms where they occured\r\n",
        "    for doc_terms in list_doc_terms:  \r\n",
        "        # This for loop is counting the amount of document a word was in\r\n",
        "        for word, value in doc_terms.items():\r\n",
        "            if 0 < value:\r\n",
        "                idf[word] += 1\r\n",
        "        \r\n",
        "    # Now we calculate idf\r\n",
        "    for word, value in idf.items():\r\n",
        "        idf[word] = math.log10(10 / float(value))\r\n",
        "    \r\n",
        "    return idf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XTmL87F5uaU"
      },
      "source": [
        "# Modified this function to return a list as dictionaries arn't needed anymore\r\n",
        "def calc_tf_idf(tf, idf, n_terms):\r\n",
        "    # Create an array that is of length of the number of unique terms\r\n",
        "    tf_idf_array = np.zeros(n_terms)\r\n",
        "    \r\n",
        "    for index, (word, value) in enumerate(tf.items()):\r\n",
        "        # Add the tfidf to the array\r\n",
        "        tf_idf_array[index] = value*idf[word]\r\n",
        "    \r\n",
        "    return tf_idf_array"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzOme9px5uks"
      },
      "source": [
        "def process_text(text_data):\r\n",
        "    # A list of all the cleaned reviews\r\n",
        "    doc_list = []\r\n",
        "    \r\n",
        "    # List of all the unique terms\r\n",
        "    unique_terms = []\r\n",
        "    \r\n",
        "    # A list of all the term frequencies\r\n",
        "    tf_list = []\r\n",
        "    \r\n",
        "    for review in text_data:\r\n",
        "        # First clean the review\r\n",
        "        clean_review = clean_text(review)\r\n",
        "        \r\n",
        "        # Keeps track of the term counts for each word\r\n",
        "        count_dict = {}\r\n",
        "        \r\n",
        "        # Now lets find the total count for each word\r\n",
        "        for token in clean_review:\r\n",
        "            if token not in count_dict:\r\n",
        "                count_dict[token] = 1\r\n",
        "            else:\r\n",
        "                count_dict[token] += 1\r\n",
        "        \r\n",
        "        # Caclulate the term frequencies for each document\r\n",
        "        tf_list.append(calc_tf(count_dict, clean_review))\r\n",
        "        \r\n",
        "        # Then add the dictionary of counts for each document to the list\r\n",
        "        doc_list.append(count_dict)\r\n",
        "        \r\n",
        "        # Then add the new unique terms\r\n",
        "        unique_terms = set(unique_terms).union(set(clean_review))\r\n",
        "    \r\n",
        "    # Calculate the inverse document frequency value\r\n",
        "    idf = calc_idf(unique_terms, doc_list)\r\n",
        "    \r\n",
        "    # This array will contain the tfidf values for each term in each review\r\n",
        "    tfidf_values = np.zeros((len(tf_list), len(unique_terms)))\r\n",
        "    \r\n",
        "    # Now we can get the TFIDF for each document\r\n",
        "    for index, term_freq in enumerate(tf_list):\r\n",
        "        # This will return an array of the tfidf values calculated.\r\n",
        "        # The length of the unique terms list is passed in so that the \r\n",
        "        # Array that is returned matches the tfidf array\r\n",
        "        tf_idf_array = calc_tf_idf(term_freq, idf, len(unique_terms))\r\n",
        "        # Add this to the overall tfidf values calculated\r\n",
        "        tfidf_values[index,:] = tf_idf_array\r\n",
        "    \r\n",
        "    return tfidf_values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUz1Vq_D6Ett"
      },
      "source": [
        "# Prepare the data\r\n",
        "def prepare_data():\r\n",
        "    print(\"=\"*50)\r\n",
        "\r\n",
        "    # Load the dataset\r\n",
        "    # imdb_df = pd.read_csv(\"IMDB Dataset.csv\")\r\n",
        "    imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\r\n",
        "    print(\"Dataset loaded\")\r\n",
        "    print(\"=\"*50)\r\n",
        "\r\n",
        "    \r\n",
        "    # Change each positive and negative value to 1 and 0 respectively    \r\n",
        "    imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\r\n",
        "    \r\n",
        "    # For testing, a much smaller dataset is going to be used\r\n",
        "    imdb_df = imdb_df.head(5000)\r\n",
        "\r\n",
        "    # Group all the negative reviews and get the first 2500\r\n",
        "    imdb_df_neg = (imdb_df[imdb_df['sentiment'] == 0])[0:2500]\r\n",
        "    # Group all the positive and get the first 2500\r\n",
        "    imdb_df_pos = imdb_df[imdb_df['sentiment'] == 1]\r\n",
        "    \r\n",
        "    test_df = pd.concat([imdb_df_neg, imdb_df_pos]) \r\n",
        "    # print(test_df)\r\n",
        "    \r\n",
        "    # .values on a column of a dataframe returns a numpy array\r\n",
        "    # This is a numpy array of all the reviews\r\n",
        "    # initial_reviews = imdb_df['review'].values\r\n",
        "    initial_reviews = test_df['review'].values\r\n",
        "    \r\n",
        "    # This is a numpy array of all the positive and negativelabels\r\n",
        "    # labels = imdb_df['sentiment'].values\r\n",
        "    labels = test_df['sentiment'].values\r\n",
        "    \r\n",
        "    print(\"Creating Feature Vector\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    start = time.time()\r\n",
        "    # Process the text data and create teh feature vector\r\n",
        "    feature_vector = process_text(initial_reviews)\r\n",
        "    end = time.time()\r\n",
        "    print(\"Feature Vector Created\")\r\n",
        "    print(f\"Execution time is {end - start} secs\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    \r\n",
        "    # Shuffle the labesl and feature vector using sklearn shuffle\r\n",
        "    feature_vector, labels = shuffle(feature_vector, labels)\r\n",
        "    \r\n",
        "    # Creating train and test data\r\n",
        "    # The splits will be 80:20 \r\n",
        "    no_samples = 0.8\r\n",
        "    \r\n",
        "    # This gets the percentage of indexes from feature vector and uses those for training\r\n",
        "    X_train = feature_vector[0:int(no_samples*len(feature_vector))]\r\n",
        "    y_train = labels[0:int(no_samples*len(labels))]\r\n",
        "    \r\n",
        "    # Go from the index that was used for training to the final\r\n",
        "    X_test = feature_vector[int(no_samples*len(feature_vector)):len(feature_vector)]\r\n",
        "    y_test = labels[int(no_samples*len(labels)):len(labels)]\r\n",
        "\r\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xJJPc-lCwmr"
      },
      "source": [
        "# Create Model Section \r\n",
        "\r\n",
        "**Exporting models:**\r\n",
        "\r\n",
        "https://autokeras.com/tutorial/export/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UMmDW00xJrU"
      },
      "source": [
        "Auto Keras has a built in text classifier that takes in strings so going to test using that?\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPmp7nYR5mZb"
      },
      "source": [
        "import autokeras as ak"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_1aYh1j5mUM",
        "outputId": "7f48a649-eaa2-4b29-e684-d0189f142272"
      },
      "source": [
        "# Load the dataset\r\n",
        "imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\r\n",
        "# print(imdb_df.head())\r\n",
        "\r\n",
        "# For testing, a much smaller dataset is going to be used\r\n",
        "# imdb_df = imdb_df.head(4000)\r\n",
        "# print(imdb_df.head())\r\n",
        "\r\n",
        "# Group all the negative reviews and get the first 25000\r\n",
        "imdb_df_neg = (imdb_df[imdb_df['sentiment'] == \"negative\"])[0:2500]\r\n",
        "# Group all the positive and get the first 25000\r\n",
        "imdb_df_pos = imdb_df[imdb_df['sentiment'] == \"positive\"][0:2500]\r\n",
        "\r\n",
        "# Combine the two split positives and negatives into one dataframe\r\n",
        "imdb_df = pd.concat([imdb_df_neg, imdb_df_pos]) \r\n",
        "\r\n",
        "# .values on a column of a dataframe returns a numpy array\r\n",
        "# This is a numpy array of all the reviews\r\n",
        "# initial_reviews = imdb_df['review'].values\r\n",
        "feature_vector = imdb_df['review'].values\r\n",
        "# print(feature_vector)\r\n",
        "\r\n",
        "# This is a numpy array of all the positive and negativelabels\r\n",
        "labels = imdb_df['sentiment'].values\r\n",
        "\r\n",
        "# Shuffle the labesl and feature vector using sklearn shuffle\r\n",
        "feature_vector, labels = shuffle(feature_vector, labels)\r\n",
        "\r\n",
        "# Creating train and test data\r\n",
        "# The splits will be 80:20 \r\n",
        "no_samples = 0.8\r\n",
        "\r\n",
        "# This gets the percentage of indexes from feature vector and uses those for training\r\n",
        "X_train = feature_vector[0:int(no_samples*len(feature_vector))]\r\n",
        "y_train = labels[0:int(no_samples*len(labels))]\r\n",
        "\r\n",
        "# Go from the index that was used for training to the final\r\n",
        "X_test = feature_vector[int(no_samples*len(feature_vector)):len(feature_vector)]\r\n",
        "y_test = labels[int(no_samples*len(labels)):len(labels)]\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(type(X_train))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000,)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHCUSQn35mQ_"
      },
      "source": [
        "# Create the text classifier\r\n",
        "# It is only going to do 1 model but change max trials for more\r\n",
        "# Has a built in project name, default is text_classifier\r\n",
        "# Trials is the amount of keras models to use, default is 1000 and might stop before reaching that number\r\n",
        "# Directory sets where search outputs will be saved\r\n",
        "# Objective is what it needs to prioritise, default is val_loss but val_accuracy can be used\r\n",
        "# Naming convention = ak_epochs_dataNo_trials\r\n",
        "clf = ak.TextClassifier(overwrite=True, project_name=\"ak_100_5000_2\", max_trials=2, directory=\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/SearchOuputs\", objective=\"val_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieccGP7y5mMZ"
      },
      "source": [
        "# Call the fit function and train the models for only 2 epochs\r\n",
        "# Leave empty for adaptive epochs\r\n",
        "# By default, auto keras will use the last 20% of training data for validation\r\n",
        "# Setting validation to 0.2 in this case to just to demonstrate\r\n",
        "print(\"Creating Fit the model\")\r\n",
        "print(\"=\"*50)\r\n",
        "start = time.time()\r\n",
        "# Default epochs is 1000\r\n",
        "clf.fit(X_train, y_train, validation_split=0.2, epochs=100)\r\n",
        "end = time.time()\r\n",
        "print(\"Model Fitting Finished\")\r\n",
        "print(f\"Execution time is {end - start} secs\")\r\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX5qeVhC5l1g"
      },
      "source": [
        "# Create some predictions\r\n",
        "predicted_y = clf.predict(X_test)\r\n",
        "\r\n",
        "# Evaluate the model used\r\n",
        "print(clf.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulaxxEW76Kyq"
      },
      "source": [
        "best_model = clf.export_model()\r\n",
        "\r\n",
        "# Naming convenction = ModelAutoKeras_epochs_dataNo_trials\r\n",
        "try:\r\n",
        "    best_model.save(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/Models/mak_100_5000_2\", save_format=\"tf\")\r\n",
        "except:\r\n",
        "    best_model.save(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/Models/mak_100_5000_2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaHxJr4U9jgN"
      },
      "source": [
        "# Test loading a model\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCqdwe5R5SNZ",
        "outputId": "7a5bde94-4e54-460a-85f7-0d1e743311e5"
      },
      "source": [
        "loaded_model = load_model(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/Models/mak_100_5000_2\", custom_objects=ak.CUSTOM_OBJECTS)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f57389ed950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH5XbZqA-oLj"
      },
      "source": [
        "predicted_y = loaded_model.predict(tf.expand_dims(X_test, -1))\r\n",
        "print(predicted_y[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ouuh9zjg-rsO",
        "outputId": "00e215ee-6054-46fc-a0f5-6bd7dcee4abf"
      },
      "source": [
        "print(loaded_model.predict(X_test)[:5])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0000000e+00]\n",
            " [4.3246618e-20]\n",
            " [3.5085404e-10]\n",
            " [1.0000000e+00]\n",
            " [2.4857304e-11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZEGdcU__tVT"
      },
      "source": [
        "for i, v in enumerate(y_test):\r\n",
        "  if y_test[i] == \"positive\":\r\n",
        "    y_test[i] = 1.0\r\n",
        "  else:\r\n",
        "    y_test[i] = 0.0"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "bzzLpbVm-vhx",
        "outputId": "7179a1dc-bcd7-472d-d2ae-4f66155a7429"
      },
      "source": [
        "print(len(X_test))\r\n",
        "print(type(y_test))\r\n",
        "print(y_test[:5])\r\n",
        "score = loaded_model.evaluate(X_test, y_test, verbose=0)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "<class 'numpy.ndarray'>\n",
            "[0.0 0.0 0.0 0.0 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d8c6a6a5aa4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                **kwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    265\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LipjLWBh6Mvt"
      },
      "source": [
        "# Old Funciton Section\r\n",
        "\r\n",
        "Now split into individual cells for ram testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSCHYar_6dB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141f2036-e5ca-4496-e302-bc9f4f36094f"
      },
      "source": [
        "\r\n",
        "def main():\r\n",
        "  # Since this is deep learning this part isn't needed\r\n",
        "  # X_train, y_train, X_test, y_test = prepare_data()\r\n",
        "\r\n",
        "  # print(len(X_train))\r\n",
        "\r\n",
        "  #==========================================\r\n",
        "  # Insert the code for running the libraries in here\r\n",
        "  import autokeras as ak\r\n",
        "\r\n",
        "  # Load the dataset\r\n",
        "  imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\r\n",
        "  # print(imdb_df.head())\r\n",
        "\r\n",
        "  # For testing, a much smaller dataset is going to be used\r\n",
        "  # imdb_df = imdb_df.head(4000)\r\n",
        "  # print(imdb_df.head())\r\n",
        "\r\n",
        "  # Group all the negative reviews and get the first 25000\r\n",
        "  imdb_df_neg = (imdb_df[imdb_df['sentiment'] == \"negative\"])[0:2500]\r\n",
        "  # Group all the positive and get the first 25000\r\n",
        "  imdb_df_pos = imdb_df[imdb_df['sentiment'] == \"positive\"][0:2500]\r\n",
        "  \r\n",
        "  # Combine the two split positives and negatives into one dataframe\r\n",
        "  imdb_df = pd.concat([imdb_df_neg, imdb_df_pos]) \r\n",
        "\r\n",
        "  # .values on a column of a dataframe returns a numpy array\r\n",
        "  # This is a numpy array of all the reviews\r\n",
        "  # initial_reviews = imdb_df['review'].values\r\n",
        "  feature_vector = imdb_df['review'].values\r\n",
        "  # print(feature_vector)\r\n",
        "\r\n",
        "  # This is a numpy array of all the positive and negativelabels\r\n",
        "  labels = imdb_df['sentiment'].values\r\n",
        "\r\n",
        "  # Shuffle the labesl and feature vector using sklearn shuffle\r\n",
        "  feature_vector, labels = shuffle(feature_vector, labels)\r\n",
        "\r\n",
        "  # Creating train and test data\r\n",
        "  # The splits will be 80:20 \r\n",
        "  no_samples = 0.8\r\n",
        "\r\n",
        "  # This gets the percentage of indexes from feature vector and uses those for training\r\n",
        "  X_train = feature_vector[0:int(no_samples*len(feature_vector))]\r\n",
        "  y_train = labels[0:int(no_samples*len(labels))]\r\n",
        "\r\n",
        "  # Go from the index that was used for training to the final\r\n",
        "  X_test = feature_vector[int(no_samples*len(feature_vector)):len(feature_vector)]\r\n",
        "  y_test = labels[int(no_samples*len(labels)):len(labels)]\r\n",
        "\r\n",
        "  print(X_train.shape)\r\n",
        "  print(type(X_train))\r\n",
        "\r\n",
        "  # Create the text classifier\r\n",
        "  # It is only going to do 1 model but change max trials for more\r\n",
        "  # Has a built in project name, default is text_classifier\r\n",
        "  # Trials is the amount of keras models to use, default is 1000 and might stop before reaching that number\r\n",
        "  # Directory sets where search outputs will be saved\r\n",
        "  # Objective is what it needs to prioritise, default is val_loss but val_accuracy can be used\r\n",
        "  # Naming convention = ak_epochs_dataNo_trials\r\n",
        "  clf = ak.TextClassifier(overwrite=True, project_name=\"ak_100_5000_2\", max_trials=2, directory=\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/SearchOuputs\", objective=\"val_loss\")\r\n",
        "\r\n",
        "  # Call the fit function and train the models for only 2 epochs\r\n",
        "  # Leave empty for adaptive epochs\r\n",
        "  # By default, auto keras will use the last 20% of training data for validation\r\n",
        "  # Setting validation to 0.2 in this case to just to demonstrate\r\n",
        "  print(\"Creating Fit the model\")\r\n",
        "  print(\"=\"*50)\r\n",
        "  start = time.time()\r\n",
        "  # Default epochs is 1000\r\n",
        "  clf.fit(X_train, y_train, validation_split=0.2, epochs=100)\r\n",
        "  end = time.time()\r\n",
        "  print(\"Model Fitting Finished\")\r\n",
        "  print(f\"Execution time is {end - start} secs\")\r\n",
        "  print(\"=\"*50)\r\n",
        "\r\n",
        "  # Create some predictions\r\n",
        "  predicted_y = clf.predict(X_test)\r\n",
        "\r\n",
        "  # Evaluate the model used\r\n",
        "  print(clf.evaluate(X_test, y_test))\r\n",
        "\r\n",
        "  # Save the model:\r\n",
        "  best_model = clf.export_model()\r\n",
        "\r\n",
        "  # Naming convenction = ModelAutoKeras_epochs_dataNo_trials\r\n",
        "  try:\r\n",
        "      best_model.save(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/Models/mak_100_5000_2\", save_format=\"tf\")\r\n",
        "  except:\r\n",
        "      best_model.save(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/Models/mak_100_5000_2.h5\")\r\n",
        "\r\n",
        "  #==========================================\r\n",
        "\r\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 2 Complete [00h 02m 06s]\n",
            "val_loss: 0.4370107650756836\n",
            "\n",
            "Best val_loss So Far: 0.3781603276729584\n",
            "Total elapsed time: 00h 07m 23s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/100\n",
            "125/125 [==============================] - 36s 281ms/step - loss: 0.6948 - accuracy: 0.5064\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.5890 - accuracy: 0.6954\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.3521 - accuracy: 0.8451\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.1725 - accuracy: 0.9389\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 0.0817 - accuracy: 0.9719\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0409 - accuracy: 0.9864\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 0.0356 - accuracy: 0.9891\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 0.0207 - accuracy: 0.9932\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0068 - accuracy: 0.9985\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0053 - accuracy: 0.9985\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 0.0014 - accuracy: 0.9996\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0017 - accuracy: 0.9995\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0015 - accuracy: 0.9999\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0027 - accuracy: 0.9989\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0018 - accuracy: 0.9995\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0017 - accuracy: 0.9996\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 7.9717e-04 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0094 - accuracy: 0.9967\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0094 - accuracy: 0.9974\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0014 - accuracy: 0.9997\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0028 - accuracy: 0.9991\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0044 - accuracy: 0.9990\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0209 - accuracy: 0.9934\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0294 - accuracy: 0.9926\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0114 - accuracy: 0.9967\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0029 - accuracy: 0.9998\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0029 - accuracy: 0.9993\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0047 - accuracy: 0.9988\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 8.7707e-04 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0010 - accuracy: 0.9998\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 5.6252e-04 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 8.3943e-04 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 26s 210ms/step - loss: 3.8026e-04 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0019 - accuracy: 0.9993\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0025 - accuracy: 0.9993\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0033 - accuracy: 0.9984\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0015 - accuracy: 0.9994\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0028 - accuracy: 0.9989\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 5.2783e-04 - accuracy: 0.9999\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0010 - accuracy: 0.9998\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 0.0115 - accuracy: 0.9961\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0231 - accuracy: 0.9932\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0092 - accuracy: 0.9969\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0023 - accuracy: 0.9995\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0010 - accuracy: 0.9996\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 9.9581e-04 - accuracy: 0.9998\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0021 - accuracy: 0.9988\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0052 - accuracy: 0.9987\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0028 - accuracy: 0.9991\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0013 - accuracy: 0.9998\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 2.2963e-04 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0044 - accuracy: 0.9986\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 2.7010e-04 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0048 - accuracy: 0.9988\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 2.7017e-04 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0038 - accuracy: 0.9978\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0134 - accuracy: 0.9982\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0038 - accuracy: 0.9984\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 0.0089 - accuracy: 0.9975\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0147 - accuracy: 0.9948\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0105 - accuracy: 0.9979\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0016 - accuracy: 0.9992\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 6.0228e-04 - accuracy: 0.9997\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 5.9072e-04 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0010 - accuracy: 0.9997\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 0.0022 - accuracy: 0.9994\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 1.2309e-04 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 26s 206ms/step - loss: 4.6266e-04 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0011 - accuracy: 0.9994\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 8.3480e-04 - accuracy: 0.9994\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 1.4172e-04 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 26s 212ms/step - loss: 6.5948e-04 - accuracy: 0.9998\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 6.0269e-04 - accuracy: 0.9997\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 7.3753e-05 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 7.8664e-05 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 7.4345e-04 - accuracy: 0.9996\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 8.8670e-04 - accuracy: 0.9999\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0027 - accuracy: 0.9986\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0079 - accuracy: 0.9966\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0058 - accuracy: 0.9993\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0102 - accuracy: 0.9975\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0069 - accuracy: 0.9983\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0093 - accuracy: 0.9979\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0030 - accuracy: 0.9986\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 8.1015e-04 - accuracy: 0.9997\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0014 - accuracy: 0.9993\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 3.2386e-04 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 4.4863e-04 - accuracy: 0.9999\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0074 - accuracy: 0.9979\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0110 - accuracy: 0.9976\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 0.0017 - accuracy: 0.9989\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0049 - accuracy: 0.9973\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 1.7682e-04 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 5.5213e-05 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 26s 209ms/step - loss: 0.0030 - accuracy: 0.9995\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 26s 207ms/step - loss: 0.0014 - accuracy: 0.9992\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 26s 208ms/step - loss: 9.5386e-04 - accuracy: 0.9994\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/SearchOuputs/ak_100_5000_2/best_model/assets\n",
            "Model Fitting Finished\n",
            "Execution time is 3055.5091545581818 secs\n",
            "==================================================\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 1.8743 - accuracy: 0.8370\n",
            "[1.8743208646774292, 0.8370000123977661]\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/Models/mak_100_5000_2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPyOxhz0acqi"
      },
      "source": [
        "First sucesssful output:\r\n",
        "Trial 2 Complete [00h 18m 08s]\r\n",
        "val_loss: 0.33026546239852905\r\n",
        "\r\n",
        "Best val_loss So Far: 0.26305750012397766\r\n",
        "Total elapsed time: 01h 01m 52s\r\n",
        "INFO:tensorflow:Oracle triggered exit\r\n",
        "Epoch 1/20\r\n",
        "1250/1250 [==============================] - 257s 205ms/step - loss: 0.5032 - accuracy: 0.7166\r\n",
        "Epoch 2/20\r\n",
        "1250/1250 [==============================] - 258s 206ms/step - loss: 0.2453 - accuracy: 0.8989\r\n",
        "Epoch 3/20\r\n",
        "1250/1250 [==============================] - 257s 206ms/step - loss: 0.1807 - accuracy: 0.9318\r\n",
        "Epoch 4/20\r\n",
        "1250/1250 [==============================] - 257s 206ms/step - loss: 0.1339 - accuracy: 0.9517\r\n",
        "Epoch 5/20\r\n",
        "1250/1250 [==============================] - 257s 206ms/step - loss: 0.1052 - accuracy: 0.9628\r\n",
        "Epoch 6/20\r\n",
        "1250/1250 [==============================] - 258s 206ms/step - loss: 0.0887 - accuracy: 0.9683\r\n",
        "Epoch 7/20\r\n",
        "1250/1250 [==============================] - 258s 206ms/step - loss: 0.0741 - accuracy: 0.9731\r\n",
        "Epoch 8/20\r\n",
        "1250/1250 [==============================] - 258s 206ms/step - loss: 0.0560 - accuracy: 0.9802\r\n",
        "Epoch 9/20\r\n",
        "1250/1250 [==============================] - 261s 208ms/step - loss: 0.0481 - accuracy: 0.9829\r\n",
        "Epoch 10/20\r\n",
        "1250/1250 [==============================] - 259s 207ms/step - loss: 0.0449 - accuracy: 0.9835\r\n",
        "Epoch 11/20\r\n",
        "1250/1250 [==============================] - 260s 208ms/step - loss: 0.0448 - accuracy: 0.9833\r\n",
        "Epoch 12/20\r\n",
        "1250/1250 [==============================] - 261s 209ms/step - loss: 0.0401 - accuracy: 0.9852\r\n",
        "Epoch 13/20\r\n",
        "1250/1250 [==============================] - 261s 209ms/step - loss: 0.0390 - accuracy: 0.9861\r\n",
        "Epoch 14/20\r\n",
        "1250/1250 [==============================] - 260s 208ms/step - loss: 0.0326 - accuracy: 0.9879\r\n",
        "Epoch 15/20\r\n",
        "1250/1250 [==============================] - 257s 206ms/step - loss: 0.0329 - accuracy: 0.9889\r\n",
        "Epoch 16/20\r\n",
        "1250/1250 [==============================] - 259s 207ms/step - loss: 0.0308 - accuracy: 0.9886\r\n",
        "Epoch 17/20\r\n",
        "1250/1250 [==============================] - 260s 208ms/step - loss: 0.0301 - accuracy: 0.9880\r\n",
        "Epoch 18/20\r\n",
        "1250/1250 [==============================] - 259s 207ms/step - loss: 0.0253 - accuracy: 0.9912\r\n",
        "Epoch 19/20\r\n",
        "1250/1250 [==============================] - 260s 208ms/step - loss: 0.0210 - accuracy: 0.9923\r\n",
        "Epoch 20/20\r\n",
        "1250/1250 [==============================] - 259s 207ms/step - loss: 0.0258 - accuracy: 0.9908\r\n",
        "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/AutoKeras/SearchOuputs/ak_20_50000_2/best_model/assets\r\n",
        "Model Fitting Finished\r\n",
        "Execution time is 8901.862411737442 secs\r\n",
        "==================================================\r\n",
        "313/313 [==============================] - 18s 56ms/step - loss: 0.6138 - accuracy: 0.8907\r\n",
        "[0.6138280630111694, 0.8906999826431274]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou_PVWsGyG90"
      },
      "source": [
        "# import os\r\n",
        "# import numpy as np\r\n",
        "# import tensorflow as tf\r\n",
        "# from tensorflow.keras.datasets import imdb\r\n",
        "# from sklearn.datasets import load_files\r\n",
        "\r\n",
        "# dataset = tf.keras.utils.get_file(\r\n",
        "#     fname=\"aclImdb.tar.gz\",\r\n",
        "#     origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\r\n",
        "#     extract=True,\r\n",
        "# )\r\n",
        "\r\n",
        "# # set path to dataset\r\n",
        "# IMDB_DATADIR = os.path.join(os.path.dirname(dataset), 'aclImdb')\r\n",
        "\r\n",
        "# classes = ['pos', 'neg']\r\n",
        "# train_data = load_files(os.path.join(IMDB_DATADIR, 'train'), shuffle=True, categories=classes)\r\n",
        "# test_data = load_files(os.path.join(IMDB_DATADIR,  'test'), shuffle=False, categories=classes)\r\n",
        "\r\n",
        "# x_train = np.array(train_data.data)\r\n",
        "# y_train = np.array(train_data.target)\r\n",
        "# x_test = np.array(test_data.data)\r\n",
        "# y_test = np.array(test_data.target)\r\n",
        "\r\n",
        "# print(x_train.shape)  # (25000,)\r\n",
        "# print(y_train.shape)  # (25000, 1)\r\n",
        "# print(x_train[0][:50])  # this film was just brilliant casting\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoth1TxYyXhI"
      },
      "source": [
        "# print(train_data.data[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}