{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of H2O MainNotebook",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fE_n6BPbCf4M5_6EGv4ZbV4pRpWNKdKh",
      "authorship_tag": "ABX9TyM/vlGxAVySD6esCdPogjL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CianOSull/AutoML_With_SA_FYP_2021/blob/H2O/MainNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_zrt8fk5AJt"
      },
      "source": [
        "# Generic Notebook for running all the libraries\r\n",
        "\r\n",
        "How this notebook works is that it contains the code\r\n",
        "for loading and cleaning the dataset.\r\n",
        "\r\n",
        "Then there is multiple branches created on the\r\n",
        "Github that include the code for running each library.\r\n",
        "\r\n",
        "E.g. MLBox branch has the code for running MLBox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJSxy1VR-mAM"
      },
      "source": [
        "# CURRENT BRANCH: H2O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxc3argyH4PH"
      },
      "source": [
        "# Install Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wFpyMaK6tMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a727410f-da5b-442d-e2ee-6967bedf9f64"
      },
      "source": [
        "# Insert any install comamnds in this cell\r\n",
        "!apt-get install default-jre\r\n",
        "!java -version\r\n",
        "!pip install h2o"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "default-jre is already the newest version (2:1.11-68ubuntu1~18.04.1).\n",
            "default-jre set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "openjdk version \"11.0.10\" 2021-01-19\n",
            "OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
            "Collecting h2o\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/d4/5c07504a392e94786e7cf33554d961ac4b2863aa22a07b8579940ea1f6b5/h2o-3.32.0.4.tar.gz (164.6MB)\n",
            "\u001b[K     |████████████████████████████████| 164.6MB 55kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from h2o) (2.23.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from h2o) (0.8.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from h2o) (0.16.0)\n",
            "Collecting colorama>=0.3.8\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (1.24.3)\n",
            "Building wheels for collected packages: h2o\n",
            "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o: filename=h2o-3.32.0.4-py2.py3-none-any.whl size=164670979 sha256=1d5b58f7d665953832d767e2712f8fb846744e918eaf79ca302a7203d8aa6cf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/f4/0d/c9bb958d70c2e015c968cb91cbd7f1b486933056d422337d75\n",
            "Successfully built h2o\n",
            "Installing collected packages: colorama, h2o\n",
            "Successfully installed colorama-0.4.4 h2o-3.32.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TNIZJ6c5hcF"
      },
      "source": [
        "# Preprocessing Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njEYGcZQ42t-"
      },
      "source": [
        "# Import the necessary modules for cleaning\r\n",
        "import math\r\n",
        "import time \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sPOCEGS5Xgn"
      },
      "source": [
        "# Create the set of stopwords for cleaning text\r\n",
        "stopwords = set(w.rstrip() for w in open('/content/drive/MyDrive/CIT/FYP/ImplementationFiles/stopwords.txt'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93NNPX-A5srS",
        "outputId": "237120a4-6010-4681-8809-1740b1ea9efe"
      },
      "source": [
        "# Download the necessary parts for the NLTK module\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ptneBc5tAN"
      },
      "source": [
        "# This funciton handles celaning text\r\n",
        "def clean_text(text):\r\n",
        "    # Create the lemmatizer\r\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "    \r\n",
        "    # Get rid of non alpha characters except \"'\" as it is needed for the lemment\r\n",
        "    text = \"\".join(c for c in text if c.isalnum() or c == \" \" or \"'\")\r\n",
        "    \r\n",
        "    # Get rid of capitals\r\n",
        "    text = text.lower()\r\n",
        "    \r\n",
        "    # Tokenize the words    \r\n",
        "    # Create tokens of each word\r\n",
        "    token_text = word_tokenize(text)\r\n",
        "    \r\n",
        "    # Get rid of any piece of text that isn't over 2 characters\r\n",
        "    token_text = [t for t in token_text if len(t) > 2] \r\n",
        "    \r\n",
        "    # Put words in base form by doing lemmatization\r\n",
        "    token_text = [wordnet_lemmatizer.lemmatize(t) for t in token_text]\r\n",
        "\r\n",
        "    # Remove stopwords\r\n",
        "    token_text = [t for t in token_text if t not in stopwords]\r\n",
        "    \r\n",
        "    # Return the tokens\r\n",
        "    return token_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu5dFDM75tjR"
      },
      "source": [
        "# This function will get the term frequencies for word in the review\r\n",
        "# TF = Term I frequency in document/total words in document\r\n",
        "def calc_tf(term_count, review_corpus):\r\n",
        "    # A dictionary of all the term frequencies found\r\n",
        "    tf_freq = dict.fromkeys(term_count.keys(), 0)   \r\n",
        "    \r\n",
        "    # Review corpus is a tokenized list so the total words iteh length\r\n",
        "    total_words = len(review_corpus)\r\n",
        "    \r\n",
        "    # Calculate the term frequency for each word\r\n",
        "    for word, count in term_count.items():\r\n",
        "        tf_freq[word] = count/total_words\r\n",
        "        \r\n",
        "    return tf_freq"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i64Bq0rH5uN0"
      },
      "source": [
        "# This calcualtes the idf\r\n",
        "# IDF = log(2)*(Total number of Documents/documents frequency or documents with term)\r\n",
        "def calc_idf(unique_terms, list_doc_terms):   \r\n",
        "    # A dicitonary of all the inverse document frequencies\r\n",
        "    idf = dict.fromkeys(unique_terms, 0)\r\n",
        "    \r\n",
        "    # Basically list_doc_terms has all the documents with the term count for each word\r\n",
        "    # You go through each document count the terms where they occured\r\n",
        "    for doc_terms in list_doc_terms:  \r\n",
        "        # This for loop is counting the amount of document a word was in\r\n",
        "        for word, value in doc_terms.items():\r\n",
        "            if 0 < value:\r\n",
        "                idf[word] += 1\r\n",
        "        \r\n",
        "    # Now we calculate idf\r\n",
        "    for word, value in idf.items():\r\n",
        "        idf[word] = math.log10(10 / float(value))\r\n",
        "    \r\n",
        "    return idf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XTmL87F5uaU"
      },
      "source": [
        "# Modified this function to return a list as dictionaries arn't needed anymore\r\n",
        "def calc_tf_idf(tf, idf, n_terms):\r\n",
        "    # Create an array that is of length of the number of unique terms\r\n",
        "    tf_idf_array = np.zeros(n_terms)\r\n",
        "    \r\n",
        "    for index, (word, value) in enumerate(tf.items()):\r\n",
        "        # Add the tfidf to the array\r\n",
        "        tf_idf_array[index] = value*idf[word]\r\n",
        "    \r\n",
        "    return tf_idf_array"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzOme9px5uks"
      },
      "source": [
        "def process_text(text_data):\r\n",
        "    # A list of all the cleaned reviews\r\n",
        "    doc_list = []\r\n",
        "    \r\n",
        "    # List of all the unique terms\r\n",
        "    unique_terms = []\r\n",
        "    \r\n",
        "    # A list of all the term frequencies\r\n",
        "    tf_list = []\r\n",
        "    \r\n",
        "    for review in text_data:\r\n",
        "        # First clean the review\r\n",
        "        clean_review = clean_text(review)\r\n",
        "        \r\n",
        "        # Keeps track of the term counts for each word\r\n",
        "        count_dict = {}\r\n",
        "        \r\n",
        "        # Now lets find the total count for each word\r\n",
        "        for token in clean_review:\r\n",
        "            if token not in count_dict:\r\n",
        "                count_dict[token] = 1\r\n",
        "            else:\r\n",
        "                count_dict[token] += 1\r\n",
        "        \r\n",
        "        # Caclulate the term frequencies for each document\r\n",
        "        tf_list.append(calc_tf(count_dict, clean_review))\r\n",
        "        \r\n",
        "        # Then add the dictionary of counts for each document to the list\r\n",
        "        doc_list.append(count_dict)\r\n",
        "        \r\n",
        "        # Then add the new unique terms\r\n",
        "        unique_terms = set(unique_terms).union(set(clean_review))\r\n",
        "    \r\n",
        "    # Calculate the inverse document frequency value\r\n",
        "    idf = calc_idf(unique_terms, doc_list)\r\n",
        "    \r\n",
        "    # This array will contain the tfidf values for each term in each review\r\n",
        "    tfidf_values = np.zeros((len(tf_list), len(unique_terms)))\r\n",
        "    \r\n",
        "    # Now we can get the TFIDF for each document\r\n",
        "    for index, term_freq in enumerate(tf_list):\r\n",
        "        # This will return an array of the tfidf values calculated.\r\n",
        "        # The length of the unique terms list is passed in so that the \r\n",
        "        # Array that is returned matches the tfidf array\r\n",
        "        tf_idf_array = calc_tf_idf(term_freq, idf, len(unique_terms))\r\n",
        "        # Add this to the overall tfidf values calculated\r\n",
        "        tfidf_values[index,:] = tf_idf_array\r\n",
        "    \r\n",
        "    return tfidf_values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUz1Vq_D6Ett"
      },
      "source": [
        "# Prepare the data\r\n",
        "def prepare_data(num):\r\n",
        "    print(\"=\"*50)\r\n",
        "\r\n",
        "    # Load the dataset\r\n",
        "    # imdb_df = pd.read_csv(\"IMDB Dataset.csv\")\r\n",
        "    imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\r\n",
        "    print(\"Dataset loaded\")\r\n",
        "    print(\"=\"*50)\r\n",
        "\r\n",
        "    \r\n",
        "     # Change each positive and negative value to 1 and 0 respectively    \r\n",
        "    # imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\r\n",
        "    \r\n",
        "    # For testing, a much smaller dataset is going to be used\r\n",
        "    # imdb_df = imdb_df.head(25000)\r\n",
        "\r\n",
        "    # Group all the negative reviews and get the first 2500\r\n",
        "    # imdb_df_neg = (imdb_df[imdb_df['sentiment'] == 0])[0:num]\r\n",
        "    imdb_df_neg = (imdb_df[imdb_df['sentiment'] == \"negative\"])[0:num]\r\n",
        "    \r\n",
        "    # Group all the positive and get the first 2500\r\n",
        "    # imdb_df_pos = imdb_df[imdb_df['sentiment'] == 1][0:num]\r\n",
        "    imdb_df_pos = imdb_df[imdb_df['sentiment'] == \"positive\"][0:num]\r\n",
        "    \r\n",
        "    # Combine the two split positives and negatives into one dataframe\r\n",
        "    imdb_df = pd.concat([imdb_df_neg, imdb_df_pos]) \r\n",
        "    # print(test_df)\r\n",
        "    \r\n",
        "    # .values on a column of a dataframe returns a numpy array\r\n",
        "    # This is a numpy array of all the reviews\r\n",
        "    # initial_reviews = imdb_df['review'].values\r\n",
        "    initial_reviews = imdb_df['review'].values\r\n",
        "    \r\n",
        "    # This is a numpy array of all the positive and negativelabels\r\n",
        "    # labels = imdb_df['sentiment'].values\r\n",
        "    labels = imdb_df['sentiment'].values\r\n",
        "    \r\n",
        "    print(\"Creating Feature Vector\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    start = time.time()\r\n",
        "    # Process the text data and create teh feature vector\r\n",
        "    feature_vector = process_text(initial_reviews)\r\n",
        "    end = time.time()\r\n",
        "    print(\"Feature Vector Created\")\r\n",
        "    print(len(feature_vector))\r\n",
        "    print(f\"Execution time is {end - start} secs\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    \r\n",
        "    # Shuffle the labesl and feature vector using sklearn shuffle\r\n",
        "    feature_vector, labels = shuffle(feature_vector, labels)\r\n",
        "    \r\n",
        "    return feature_vector, labels\r\n",
        "\r\n",
        "    # # Creating train and test data\r\n",
        "    # # The splits will be 80:20 \r\n",
        "    # no_samples = 0.8\r\n",
        "    \r\n",
        "    # # This gets the percentage of indexes from feature vector and uses those for training\r\n",
        "    # X_train = feature_vector[0:int(no_samples*len(feature_vector))]\r\n",
        "    # y_train = labels[0:int(no_samples*len(labels))]\r\n",
        "    \r\n",
        "    # # Go from the index that was used for training to the final\r\n",
        "    # X_test = feature_vector[int(no_samples*len(feature_vector)):len(feature_vector)]\r\n",
        "    # y_test = labels[int(no_samples*len(labels)):len(labels)]\r\n",
        "\r\n",
        "    # return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew8tq1lj6v1L"
      },
      "source": [
        "# Create Model Section\r\n",
        "\r\n",
        "**Documentation on manipulating data for h2o**\r\n",
        "\r\n",
        "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging.html\r\n",
        "\r\n",
        "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html#automl-interface\r\n",
        "\r\n",
        "# Youtube tutorial\r\n",
        "https://github.com/srivatsan88/YouTubeLI/blob/master/H2O_AutoML.ipynb\r\n",
        "\r\n",
        "**BIG NOTE**\r\n",
        "\r\n",
        "It seems because of how much H2O automates everything, it only takes in files. Thus a possible solution for running it on the tf idf values is to make a pandas dataframe using the numpy values, then convert it to a csv file and then feed it into the h2o.\r\n",
        "\r\n",
        "There is also h2o.sklearn which allows h2o to work with sklearn and does allow it to work with numpy so maybe test that out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "HCHycJkYposd",
        "outputId": "00816313-2af6-450d-9cde-da3c5c576add"
      },
      "source": [
        "# Start up h2o\r\n",
        "import h2o\r\n",
        "h2o.init()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.10\" 2021-01-19; OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04); OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpf8w6i7_t\n",
            "  JVM stdout: /tmp/tmpf8w6i7_t/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpf8w6i7_t/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>03 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.32.0.4</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>23 days </td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_j3xh90</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.180 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>accepting new members, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>H2O_API_Extensions:</td>\n",
              "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.7.10 final</td></tr></table></div>"
            ],
            "text/plain": [
              "--------------------------  ------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         03 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.32.0.4\n",
              "H2O_cluster_version_age:    23 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_j3xh90\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.180 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         accepting new members, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
              "H2O_internal_security:      False\n",
              "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
              "Python_version:             3.7.10 final\n",
              "--------------------------  ------------------------------------------------------------------"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n_O4FKxpqpi"
      },
      "source": [
        "# # Load the dataset\r\n",
        "# # imdb_df = pd.read_csv(\"IMDB Dataset.csv\")\r\n",
        "# imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\r\n",
        "# print(\"Dataset loaded\")\r\n",
        "# print(\"=\"*50)\r\n",
        "\r\n",
        "# # Change each positive and negative value to 1 and 0 respectively    \r\n",
        "# # imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\r\n",
        "\r\n",
        "# # For testing, a much smaller dataset is going to be used\r\n",
        "# # imdb_df = imdb_df.head(25000)\r\n",
        "\r\n",
        "# # Group all the negative reviews and get the first 2500\r\n",
        "# # imdb_df_neg = (imdb_df[imdb_df['sentiment'] == 0])[0:100]\r\n",
        "# imdb_df_neg = (imdb_df[imdb_df['sentiment'] == \"negative\"])[0:100]\r\n",
        "\r\n",
        "# # Group all the positive and get the first 2500\r\n",
        "# # imdb_df_pos = imdb_df[imdb_df['sentiment'] == 1][0:100]\r\n",
        "# imdb_df_pos = imdb_df[imdb_df['sentiment'] == \"positive\"][0:100]\r\n",
        "\r\n",
        "# # Combine the two split positives and negatives into one dataframe\r\n",
        "# imdb_df = pd.concat([imdb_df_neg, imdb_df_pos]) \r\n",
        "\r\n",
        "# print(len(imdb_df))\r\n",
        "# print(\"=\"*50)\r\n",
        "# # Shuffle dataframe\r\n",
        "# print(len(imdb_df.sample(frac=1)))\r\n",
        "\r\n",
        "# print(imdb_df.columns.to_list())\r\n",
        "# print(type(imdb_df.columns.to_list()))\r\n",
        "# h2o_imdb_df = h2o.H2OFrame(imdb_df)\r\n",
        "# h2o_imdb_df.describe()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LRXFLs7px6Q"
      },
      "source": [
        "# Create a train, test and validation split\r\n",
        "# This will create a train split of 70% and test and validation split of 15% each\r\n",
        "# imdb_train, imdb_test, imdb_valid = h2o_imdb_df.split_frame(ratios=[0.7, 0.15])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmA3g5-m6i7F",
        "outputId": "5af9c320-6d4c-40a2-dcec-c11f6b5cfc4c"
      },
      "source": [
        "# X_train, y_train, X_test, y_test = prepare_data()\r\n",
        "# num = 100 worked\r\n",
        "feature_vector, labels = prepare_data(500)\r\n",
        "\r\n",
        "# Convert feature_vectors into a pandas dataframe of \r\n",
        "# term frequency inverse document frequency of each word\r\n",
        "tfidf_tf = pd.DataFrame(feature_vector)\r\n",
        "\r\n",
        "# Add the labels\r\n",
        "tfidf_tf['labels'] = labels\r\n",
        "\r\n",
        "#==========================================\r\n",
        "# Insert the code for running the libraries in here\r\n",
        "h2o_tfidf = h2o.H2OFrame(tfidf_tf)\r\n",
        "# Set labels to be a categorical field\r\n",
        "h2o_tfidf['labels'] = h2o_tfidf['labels'].asfactor()\r\n",
        "# print(h2o_tfidf.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Dataset loaded\n",
            "==================================================\n",
            "Creating Feature Vector\n",
            "==================================================\n",
            "Feature Vector Created\n",
            "1000\n",
            "Execution time is 5.413161039352417 secs\n",
            "==================================================\n",
            "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Hvfl1Z6n_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82773e78-d174-4e4c-f715-0d36532f5abc"
      },
      "source": [
        "y = \"labels\"\r\n",
        "x = h2o_tfidf.columns\r\n",
        "x.remove(y)\r\n",
        "\r\n",
        "h2o_tfidf_train, h2o_tfidf_test, h2o_tfidf_valid = h2o_tfidf.split_frame(ratios=[0.7, 0.15])\r\n",
        "\r\n",
        "# exceptions_split = []\r\n",
        "\r\n",
        "# for i in range(3):\r\n",
        "#   # if (!success):\r\n",
        "#   try:   \r\n",
        "#     h2o_tfidf_train, h2o_tfidf_test, h2o_tfidf_valid = h2o_tfidf.split_frame(ratios=[0.7, 0.15])\r\n",
        "\r\n",
        "#     # Break is here because if it doesnt finish with an error, the models are done    \r\n",
        "#     break\r\n",
        "#     # success = True\r\n",
        "#   except Exception as e:\r\n",
        "#     # success = False\r\n",
        "#     exceptions.append(e)\r\n",
        "\r\n",
        "# print(len(exceptions_model))\r\n",
        "\r\n",
        "print(\"Splits created\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splits created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdT50VPC6p2Z"
      },
      "source": [
        "from h2o.automl import H2OAutoML\r\n",
        "\r\n",
        "# This is using 10 max models, so it will only create 10\r\n",
        "# You can also tell it to not try to use stacked ensemble.\r\n",
        "# In this initial test stacked ensemble will be removed to keep it simple\r\n",
        "# though deep learning will be allowed since it is good on text data\r\n",
        "# You can also set timeboxes for how long models can run\r\n",
        "# Verbosity sets whetehr it gives out information while running\r\n",
        "# nfolds is for validation, by default it is set to 5 and will create validation splits\r\n",
        "# set nfolds to 0 if you have created your own validation split\r\n",
        "# Model names go like this h2o_datasize_MaxModels_Time_Seed\r\n",
        "h2o_model = H2OAutoML(max_models = 10, seed = 5, exclude_algos = [\"StackedEnsemble\", \"DeepLearning\"], verbosity=\"info\", nfolds=0, project_name=\"h2o_1000_10_na_5\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkwNC6a66ziI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb50ed4-2c08-4ffa-e37c-a56411f95cf8"
      },
      "source": [
        "exceptions_model = []\r\n",
        "\r\n",
        "# success = False\r\n",
        "\r\n",
        "# The server seems to fail sometiems but saw a stack overflow which I think\r\n",
        "# said that by running it 3 times, it just works\r\n",
        "# Tried it once and now and succeded but then ran again so set it to only run once\r\n",
        "for i in range(3):\r\n",
        "  # if (!success):\r\n",
        "  try:   \r\n",
        "    h2o_model.train(x = x, y = y, training_frame = h2o_tfidf_train, validation_frame=h2o_tfidf_valid)\r\n",
        "\r\n",
        "    # Break is here because if it doesnt finish with an error, the models are done    \r\n",
        "    break\r\n",
        "    # success = True\r\n",
        "  except Exception as e:\r\n",
        "    # success = False\r\n",
        "    exceptions_model.append(e)\r\n",
        "\r\n",
        "print(len(exceptions_model))\r\n",
        "# print(exceptions_model[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AutoML progress: |\n",
            "22:18:44.533: Project: h2o_1000_10_na_5\n",
            "22:18:44.537: Cross-validation disabled by user: no fold column nor nfolds > 1.\n",
            "22:18:44.557: Setting stopping tolerance adaptively based on the training frame: 0.037582301400141446\n",
            "22:18:44.558: Build control seed: 5\n",
            "22:18:44.558: Since cross-validation is disabled, and no leaderboard frame was provided, automatically split the training data into training and leaderboard frames in the ratio 90/10\n",
            "22:19:00.463: training frame: Frame key: automl_training_py_3_sid_8a30    cols: 18198    rows: 631  chunks: 18    size: 26768161  checksum: 343775478583807757\n",
            "22:19:00.530: validation frame: Frame key: py_5_sid_8a30    cols: 18198    rows: 141  chunks: 18    size: 26320998  checksum: 4702055585537513231\n",
            "22:19:03.355: leaderboard frame: Frame key: automl_leaderboard_py_3_sid_8a30    cols: 18198    rows: 77  chunks: 18    size: 26271768  checksum: 3592934798136410200\n",
            "22:19:03.355: blending frame: NULL\n",
            "22:19:03.355: response column: labels\n",
            "22:19:03.355: fold column: null\n",
            "22:19:03.355: weights column: null\n",
            "22:19:03.782: Loading execution steps: [{XGBoost : defaults}, {GLM : defaults}, {DRF : [def_1]}, {GBM : defaults}, {DeepLearning : defaults}, {DRF : [XRT]}, {XGBoost : grids}, {GBM : grids}, {DeepLearning : grids}, {GBM : [lr_annealing]}, {XGBoost : [lr_search]}, {StackedEnsemble : defaults}]\n",
            "22:19:03.818: Disabling Algo: DeepLearning as requested by the user.\n",
            "22:19:03.819: Disabling Algo: StackedEnsemble as requested by the user.\n",
            "22:19:03.830: AutoML job created: 2021.02.24 22:18:44.528\n",
            "22:19:03.830: AutoML build started: 2021.02.24 22:19:03.830\n",
            "22:19:03.869: AutoML: starting XGBoost_1_AutoML_20210224_221844 model training\n",
            "22:19:07.492: New leader: XGBoost_1_AutoML_20210224_221844, auc: 0.5506155950752394\n",
            "22:19:07.493: AutoML: starting XGBoost_2_AutoML_20210224_221844 model training\n",
            "22:19:09.928: New leader: XGBoost_2_AutoML_20210224_221844, auc: 0.573187414500684\n",
            "22:19:09.928: AutoML: starting XGBoost_3_AutoML_20210224_221844 model training\n",
            "22:19:12.414: AutoML: starting GLM_1_AutoML_20210224_221844 model training\n",
            "22:19:17.759: AutoML: starting DRF_1_AutoML_20210224_221844 model training\n",
            "22:19:29.478: AutoML: starting GBM_1_AutoML_20210224_221844 model training\n",
            "22:19:38.767: AutoML: starting GBM_2_AutoML_20210224_221844 model training\n",
            "22:19:47.145: AutoML: starting GBM_3_AutoML_20210224_221844 model training\n",
            "22:19:54.390: AutoML: starting GBM_4_AutoML_20210224_221844 model training\n",
            "22:20:03.772: AutoML: starting GBM_5_AutoML_20210224_221844 model training\n",
            "22:20:09.98: Skipping StackedEnsemble 'best' due to the exclude_algos option.\n",
            "22:20:09.99: Skipping StackedEnsemble 'all' due to the exclude_algos option.\n",
            "22:20:09.101: Skipping StackedEnsemble 'monotonic' due to the exclude_algos option.\n",
            "22:20:09.101: Actual modeling steps: [{XGBoost : [def_1 (10), def_2 (10), def_3 (10)]}, {GLM : [def_1 (10)]}, {DRF : [def_1 (10)]}, {GBM : [def_1 (10), def_2 (10), def_3 (10), def_4 (10), def_5 (10)]}]\n",
            "22:20:09.101: AutoML build stopped: 2021.02.24 22:20:09.101\n",
            "22:20:09.101: AutoML build done: built 10 models\n",
            "22:20:09.101: AutoML duration:  1 min  5.271 sec\n",
            "22:20:31.443: Project: h2o_1000_10_na_5\n",
            "22:20:31.443: Cross-validation disabled by user: no fold column nor nfolds > 1.\n",
            "22:20:31.444: Setting stopping tolerance adaptively based on the training frame: 0.037582301400141446\n",
            "22:20:31.444: Build control seed: 5\n",
            "22:20:31.444: Since cross-validation is disabled, and no leaderboard frame was provided, automatically split the training data into training and leaderboard frames in the ratio 90/10\n",
            "22:20:46.614: training frame: Frame key: automl_training_py_3_sid_8a30    cols: 18198    rows: 631  chunks: 18    size: 26768161  checksum: 343775478583807757\n",
            "22:20:46.687: validation frame: Frame key: py_5_sid_8a30    cols: 18198    rows: 141  chunks: 18    size: 26320998  checksum: 4702055585537513231\n",
            "22:20:49.502: leaderboard frame: Frame key: automl_leaderboard_py_3_sid_8a30    cols: 18198    rows: 77  chunks: 18    size: 26271768  checksum: 3592934798136410200\n",
            "22:20:49.502: blending frame: NULL\n",
            "22:20:49.502: response column: labels\n",
            "22:20:49.503: fold column: null\n",
            "22:20:49.503: weights column: null\n",
            "22:20:49.517: New models will be added to existing leaderboard h2o_1000_10_na_5@@labels (leaderboard frame=automl_leaderboard_py_3_sid_8a30) with already 10 models.\n",
            "22:20:49.520: Loading execution steps: [{XGBoost : defaults}, {GLM : defaults}, {DRF : [def_1]}, {GBM : defaults}, {DeepLearning : defaults}, {DRF : [XRT]}, {XGBoost : grids}, {GBM : grids}, {DeepLearning : grids}, {GBM : [lr_annealing]}, {XGBoost : [lr_search]}, {StackedEnsemble : defaults}]\n",
            "22:20:49.520: Disabling Algo: DeepLearning as requested by the user.\n",
            "22:20:49.520: Disabling Algo: StackedEnsemble as requested by the user.\n",
            "22:20:49.521: AutoML job created: 2021.02.24 22:20:31.439\n",
            "22:20:49.522: AutoML build started: 2021.02.24 22:20:49.522\n",
            "22:20:49.522: AutoML: starting XGBoost_1_AutoML_20210224_222031 model training\n",
            "\n",
            "█\n",
            "22:20:53.848: AutoML: starting XGBoost_2_AutoML_20210224_222031 model training\n",
            "\n",
            "██\n",
            "22:20:57.266: AutoML: starting XGBoost_3_AutoML_20210224_222031 model training\n",
            "22:21:00.633: AutoML: starting GLM_1_AutoML_20210224_222031 model training\n",
            "\n",
            "██\n",
            "22:21:03.993: AutoML: starting DRF_1_AutoML_20210224_222031 model training\n",
            "\n",
            "███████\n",
            "22:21:15.402: AutoML: starting GBM_1_AutoML_20210224_222031 model training\n",
            "\n",
            "██\n",
            "22:21:23.751: AutoML: starting GBM_2_AutoML_20210224_222031 model training\n",
            "\n",
            "█\n",
            "22:21:32.99: AutoML: starting GBM_3_AutoML_20210224_222031 model training\n",
            "\n",
            "███\n",
            "22:21:40.457: AutoML: starting GBM_4_AutoML_20210224_222031 model training\n",
            "\n",
            "████\n",
            "22:21:51.789: AutoML: starting GBM_5_AutoML_20210224_222031 model training\n",
            "\n",
            "██████████████████████████████████| 100%\n",
            "\n",
            "22:21:56.85: Skipping StackedEnsemble 'best' due to the exclude_algos option.\n",
            "22:21:56.86: Skipping StackedEnsemble 'all' due to the exclude_algos option.\n",
            "22:21:56.86: Skipping StackedEnsemble 'monotonic' due to the exclude_algos option.\n",
            "22:21:56.86: Actual modeling steps: [{XGBoost : [def_1 (10), def_2 (10), def_3 (10)]}, {GLM : [def_1 (10)]}, {DRF : [def_1 (10)]}, {GBM : [def_1 (10), def_2 (10), def_3 (10), def_4 (10), def_5 (10)]}]\n",
            "22:21:56.86: AutoML build stopped: 2021.02.24 22:21:56.86\n",
            "22:21:56.86: AutoML build done: built 10 models\n",
            "22:21:56.86: AutoML duration:  1 min  6.564 sec\n",
            "\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud1dKzE261oI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "a50be552-1ffd-400b-ac70-1217f3991c59"
      },
      "source": [
        "lb = h2o_model.leaderboard\r\n",
        "  \r\n",
        "lb.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>model_id                        </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>XGBoost_2_AutoML_20210224_221844</td><td style=\"text-align: right;\">0.573187</td><td style=\"text-align: right;\"> 0.775198</td><td style=\"text-align: right;\">0.581637</td><td style=\"text-align: right;\">              0.452804</td><td style=\"text-align: right;\">0.523931</td><td style=\"text-align: right;\">0.274503</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_20210224_222031</td><td style=\"text-align: right;\">0.573187</td><td style=\"text-align: right;\"> 0.775198</td><td style=\"text-align: right;\">0.581637</td><td style=\"text-align: right;\">              0.452804</td><td style=\"text-align: right;\">0.523931</td><td style=\"text-align: right;\">0.274503</td></tr>\n",
              "<tr><td>XGBoost_1_AutoML_20210224_222031</td><td style=\"text-align: right;\">0.550616</td><td style=\"text-align: right;\"> 0.804332</td><td style=\"text-align: right;\">0.582954</td><td style=\"text-align: right;\">              0.402531</td><td style=\"text-align: right;\">0.532957</td><td style=\"text-align: right;\">0.284043</td></tr>\n",
              "<tr><td>XGBoost_1_AutoML_20210224_221844</td><td style=\"text-align: right;\">0.550616</td><td style=\"text-align: right;\"> 0.804332</td><td style=\"text-align: right;\">0.582954</td><td style=\"text-align: right;\">              0.402531</td><td style=\"text-align: right;\">0.532957</td><td style=\"text-align: right;\">0.284043</td></tr>\n",
              "<tr><td>GBM_1_AutoML_20210224_221844    </td><td style=\"text-align: right;\">0.52052 </td><td style=\"text-align: right;\"> 0.713749</td><td style=\"text-align: right;\">0.584197</td><td style=\"text-align: right;\">              0.455882</td><td style=\"text-align: right;\">0.510511</td><td style=\"text-align: right;\">0.260621</td></tr>\n",
              "<tr><td>GBM_1_AutoML_20210224_222031    </td><td style=\"text-align: right;\">0.52052 </td><td style=\"text-align: right;\"> 0.713749</td><td style=\"text-align: right;\">0.584197</td><td style=\"text-align: right;\">              0.455882</td><td style=\"text-align: right;\">0.510511</td><td style=\"text-align: right;\">0.260621</td></tr>\n",
              "<tr><td>GBM_3_AutoML_20210224_221844    </td><td style=\"text-align: right;\">0.519836</td><td style=\"text-align: right;\"> 0.726438</td><td style=\"text-align: right;\">0.536688</td><td style=\"text-align: right;\">              0.423393</td><td style=\"text-align: right;\">0.515188</td><td style=\"text-align: right;\">0.265419</td></tr>\n",
              "<tr><td>GBM_3_AutoML_20210224_222031    </td><td style=\"text-align: right;\">0.519836</td><td style=\"text-align: right;\"> 0.726438</td><td style=\"text-align: right;\">0.536688</td><td style=\"text-align: right;\">              0.423393</td><td style=\"text-align: right;\">0.515188</td><td style=\"text-align: right;\">0.265419</td></tr>\n",
              "<tr><td>GBM_5_AutoML_20210224_221844    </td><td style=\"text-align: right;\">0.5171  </td><td style=\"text-align: right;\"> 0.70004 </td><td style=\"text-align: right;\">0.547834</td><td style=\"text-align: right;\">              0.482216</td><td style=\"text-align: right;\">0.503217</td><td style=\"text-align: right;\">0.253227</td></tr>\n",
              "<tr><td>GBM_5_AutoML_20210224_222031    </td><td style=\"text-align: right;\">0.5171  </td><td style=\"text-align: right;\"> 0.70004 </td><td style=\"text-align: right;\">0.547834</td><td style=\"text-align: right;\">              0.482216</td><td style=\"text-align: right;\">0.503217</td><td style=\"text-align: right;\">0.253227</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8O6we9etk45",
        "outputId": "bd28044d-7fe3-4f33-b2b2-8c81ccbb1fe9"
      },
      "source": [
        "# Using the best model make predictions\r\n",
        "h2o_tfidf_pred = h2o_model.leader.predict(h2o_tfidf_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xgboost prediction progress: |████████████████████████████████████████████| 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "XnrIIb1VuEs0",
        "outputId": "2f29ce9b-a60d-4927-bd72-337a41428538"
      },
      "source": [
        "# First 10 predictions\r\n",
        "h2o_tfidf_pred.head()\r\n",
        "# Shows the probabilities for each one"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>predict  </th><th style=\"text-align: right;\">  negative</th><th style=\"text-align: right;\">  positive</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.871038</td><td style=\"text-align: right;\">  0.128962</td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.81946 </td><td style=\"text-align: right;\">  0.18054 </td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.531022</td><td style=\"text-align: right;\">  0.468978</td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.57719 </td><td style=\"text-align: right;\">  0.42281 </td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.727484</td><td style=\"text-align: right;\">  0.272516</td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.733237</td><td style=\"text-align: right;\">  0.266763</td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.352137</td><td style=\"text-align: right;\">  0.647863</td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.708516</td><td style=\"text-align: right;\">  0.291484</td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.501457</td><td style=\"text-align: right;\">  0.498543</td></tr>\n",
              "<tr><td>positive </td><td style=\"text-align: right;\">  0.396234</td><td style=\"text-align: right;\">  0.603766</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x6dqL9STud9j",
        "outputId": "7cc71903-0fae-446a-d6ff-343c27bfb25e"
      },
      "source": [
        "# This is a performance report\r\n",
        "h2o_model.leader.model_performance(h2o_tfidf_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ModelMetricsBinomial: xgboost\n",
            "** Reported on test data. **\n",
            "\n",
            "MSE: 0.31978510463995274\n",
            "RMSE: 0.565495450591738\n",
            "LogLoss: 0.8671292979918205\n",
            "Mean Per-Class Error: 0.45664534470504625\n",
            "AUC: 0.4566453447050462\n",
            "AUCPR: 0.47080266284168487\n",
            "Gini: -0.0867093105899076\n",
            "\n",
            "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.056861039251089096: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "      <th>Error</th>\n",
              "      <th>Rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>1.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>0.9881</td>\n",
              "      <td>(83.0/84.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>(0.0/67.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Total</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.5497</td>\n",
              "      <td>(83.0/151.0)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             negative  positive   Error           Rate\n",
              "0  negative       1.0      83.0  0.9881    (83.0/84.0)\n",
              "1  positive       0.0      67.0     0.0     (0.0/67.0)\n",
              "2     Total       1.0     150.0  0.5497   (83.0/151.0)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Maximum Metrics: Maximum metrics at their respective thresholds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>metric</th>\n",
              "      <th>threshold</th>\n",
              "      <th>value</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>max f1</td>\n",
              "      <td>0.056861</td>\n",
              "      <td>0.617512</td>\n",
              "      <td>149.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>max f2</td>\n",
              "      <td>0.056861</td>\n",
              "      <td>0.801435</td>\n",
              "      <td>149.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>max f0point5</td>\n",
              "      <td>0.056861</td>\n",
              "      <td>0.502249</td>\n",
              "      <td>149.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>max accuracy</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.589404</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>max precision</td>\n",
              "      <td>0.916136</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>max recall</td>\n",
              "      <td>0.056861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>149.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>max specificity</td>\n",
              "      <td>0.916136</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>max absolute_mcc</td>\n",
              "      <td>0.497644</td>\n",
              "      <td>0.209723</td>\n",
              "      <td>82.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>max min_per_class_accuracy</td>\n",
              "      <td>0.507783</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>77.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>max mean_per_class_accuracy</td>\n",
              "      <td>0.857004</td>\n",
              "      <td>0.543355</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>max tns</td>\n",
              "      <td>0.916136</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>max fns</td>\n",
              "      <td>0.916136</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>max fps</td>\n",
              "      <td>0.040599</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>max tps</td>\n",
              "      <td>0.056861</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>149.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>max tnr</td>\n",
              "      <td>0.916136</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>max fnr</td>\n",
              "      <td>0.916136</td>\n",
              "      <td>0.985075</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>max fpr</td>\n",
              "      <td>0.040599</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>max tpr</td>\n",
              "      <td>0.056861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>149.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         metric  threshold      value    idx\n",
              "0                        max f1   0.056861   0.617512  149.0\n",
              "1                        max f2   0.056861   0.801435  149.0\n",
              "2                  max f0point5   0.056861   0.502249  149.0\n",
              "3                  max accuracy   0.857004   0.589404   12.0\n",
              "4                 max precision   0.916136   1.000000    0.0\n",
              "5                    max recall   0.056861   1.000000  149.0\n",
              "6               max specificity   0.916136   1.000000    0.0\n",
              "7              max absolute_mcc   0.497644   0.209723   82.0\n",
              "8    max min_per_class_accuracy   0.507783   0.404762   77.0\n",
              "9   max mean_per_class_accuracy   0.857004   0.543355   12.0\n",
              "10                      max tns   0.916136  84.000000    0.0\n",
              "11                      max fns   0.916136  66.000000    0.0\n",
              "12                      max fps   0.040599  84.000000  150.0\n",
              "13                      max tps   0.056861  67.000000  149.0\n",
              "14                      max tnr   0.916136   1.000000    0.0\n",
              "15                      max fnr   0.916136   0.985075    0.0\n",
              "16                      max fpr   0.040599   1.000000  150.0\n",
              "17                      max tpr   0.056861   1.000000  149.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Gains/Lift Table: Avg response rate: 44.37 %, avg score: 51.79 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>group</th>\n",
              "      <th>cumulative_data_fraction</th>\n",
              "      <th>lower_threshold</th>\n",
              "      <th>lift</th>\n",
              "      <th>cumulative_lift</th>\n",
              "      <th>response_rate</th>\n",
              "      <th>score</th>\n",
              "      <th>cumulative_response_rate</th>\n",
              "      <th>cumulative_score</th>\n",
              "      <th>capture_rate</th>\n",
              "      <th>cumulative_capture_rate</th>\n",
              "      <th>gain</th>\n",
              "      <th>cumulative_gain</th>\n",
              "      <th>kolmogorov_smirnov</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.013245</td>\n",
              "      <td>0.910680</td>\n",
              "      <td>2.253731</td>\n",
              "      <td>2.253731</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.913557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.913557</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>125.373134</td>\n",
              "      <td>125.373134</td>\n",
              "      <td>0.029851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.026490</td>\n",
              "      <td>0.909937</td>\n",
              "      <td>1.126866</td>\n",
              "      <td>1.690299</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.910159</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.911858</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>12.686567</td>\n",
              "      <td>69.029851</td>\n",
              "      <td>0.032871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.033113</td>\n",
              "      <td>0.903634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.352239</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.905928</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.910672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>35.223881</td>\n",
              "      <td>0.020967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.046358</td>\n",
              "      <td>0.898739</td>\n",
              "      <td>1.126866</td>\n",
              "      <td>1.287846</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.900039</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.907634</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>12.686567</td>\n",
              "      <td>28.784648</td>\n",
              "      <td>0.023987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.052980</td>\n",
              "      <td>0.892521</td>\n",
              "      <td>2.253731</td>\n",
              "      <td>1.408582</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.896967</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.906301</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.074627</td>\n",
              "      <td>125.373134</td>\n",
              "      <td>40.858209</td>\n",
              "      <td>0.038913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0.105960</td>\n",
              "      <td>0.825250</td>\n",
              "      <td>1.408582</td>\n",
              "      <td>1.408582</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.859270</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.882785</td>\n",
              "      <td>0.074627</td>\n",
              "      <td>0.149254</td>\n",
              "      <td>40.858209</td>\n",
              "      <td>40.858209</td>\n",
              "      <td>0.077825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0.152318</td>\n",
              "      <td>0.790667</td>\n",
              "      <td>0.643923</td>\n",
              "      <td>1.175860</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.800652</td>\n",
              "      <td>0.521739</td>\n",
              "      <td>0.857788</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.179104</td>\n",
              "      <td>-35.607676</td>\n",
              "      <td>17.585983</td>\n",
              "      <td>0.048152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0.205298</td>\n",
              "      <td>0.743851</td>\n",
              "      <td>0.845149</td>\n",
              "      <td>1.090515</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.761889</td>\n",
              "      <td>0.483871</td>\n",
              "      <td>0.833040</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>0.223881</td>\n",
              "      <td>-15.485075</td>\n",
              "      <td>9.051517</td>\n",
              "      <td>0.033404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>0.304636</td>\n",
              "      <td>0.651912</td>\n",
              "      <td>0.751244</td>\n",
              "      <td>0.979883</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.695316</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>0.788130</td>\n",
              "      <td>0.074627</td>\n",
              "      <td>0.298507</td>\n",
              "      <td>-24.875622</td>\n",
              "      <td>-2.011681</td>\n",
              "      <td>-0.011016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>0.403974</td>\n",
              "      <td>0.600331</td>\n",
              "      <td>0.450746</td>\n",
              "      <td>0.849768</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.625274</td>\n",
              "      <td>0.377049</td>\n",
              "      <td>0.748083</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>0.343284</td>\n",
              "      <td>-54.925373</td>\n",
              "      <td>-15.023244</td>\n",
              "      <td>-0.109097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>0.503311</td>\n",
              "      <td>0.514591</td>\n",
              "      <td>0.600995</td>\n",
              "      <td>0.800668</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.552612</td>\n",
              "      <td>0.355263</td>\n",
              "      <td>0.709504</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>0.402985</td>\n",
              "      <td>-39.900498</td>\n",
              "      <td>-19.933229</td>\n",
              "      <td>-0.180348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>0.602649</td>\n",
              "      <td>0.468010</td>\n",
              "      <td>1.201990</td>\n",
              "      <td>0.866820</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.487901</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.672976</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.522388</td>\n",
              "      <td>20.199005</td>\n",
              "      <td>-13.318025</td>\n",
              "      <td>-0.144279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>0.701987</td>\n",
              "      <td>0.376304</td>\n",
              "      <td>1.201990</td>\n",
              "      <td>0.914250</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.417202</td>\n",
              "      <td>0.405660</td>\n",
              "      <td>0.636781</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.641791</td>\n",
              "      <td>20.199005</td>\n",
              "      <td>-8.575049</td>\n",
              "      <td>-0.108209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>0.801325</td>\n",
              "      <td>0.288821</td>\n",
              "      <td>1.502488</td>\n",
              "      <td>0.987172</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.322607</td>\n",
              "      <td>0.438017</td>\n",
              "      <td>0.597834</td>\n",
              "      <td>0.149254</td>\n",
              "      <td>0.791045</td>\n",
              "      <td>50.248756</td>\n",
              "      <td>-1.282842</td>\n",
              "      <td>-0.018479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>0.900662</td>\n",
              "      <td>0.203634</td>\n",
              "      <td>0.901493</td>\n",
              "      <td>0.977722</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.253161</td>\n",
              "      <td>0.433824</td>\n",
              "      <td>0.559819</td>\n",
              "      <td>0.089552</td>\n",
              "      <td>0.880597</td>\n",
              "      <td>-9.850746</td>\n",
              "      <td>-2.227831</td>\n",
              "      <td>-0.036070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.040599</td>\n",
              "      <td>1.201990</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.137876</td>\n",
              "      <td>0.443709</td>\n",
              "      <td>0.517904</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.199005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    group  cumulative_data_fraction  ...  cumulative_gain  kolmogorov_smirnov\n",
              "0       1                  0.013245  ...       125.373134            0.029851\n",
              "1       2                  0.026490  ...        69.029851            0.032871\n",
              "2       3                  0.033113  ...        35.223881            0.020967\n",
              "3       4                  0.046358  ...        28.784648            0.023987\n",
              "4       5                  0.052980  ...        40.858209            0.038913\n",
              "5       6                  0.105960  ...        40.858209            0.077825\n",
              "6       7                  0.152318  ...        17.585983            0.048152\n",
              "7       8                  0.205298  ...         9.051517            0.033404\n",
              "8       9                  0.304636  ...        -2.011681           -0.011016\n",
              "9      10                  0.403974  ...       -15.023244           -0.109097\n",
              "10     11                  0.503311  ...       -19.933229           -0.180348\n",
              "11     12                  0.602649  ...       -13.318025           -0.144279\n",
              "12     13                  0.701987  ...        -8.575049           -0.108209\n",
              "13     14                  0.801325  ...        -1.282842           -0.018479\n",
              "14     15                  0.900662  ...        -2.227831           -0.036070\n",
              "15     16                  1.000000  ...         0.000000            0.000000\n",
              "\n",
              "[16 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "OWAEDnCQu4dB",
        "outputId": "815c6de1-f36d-4064-ff2d-1ac49e35ecc1"
      },
      "source": [
        "best_model = h2o.get_model(h2o_model.leader.model_id)\r\n",
        "best_model.confusion_matrix()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.46533888578414917: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "      <th>Error</th>\n",
              "      <th>Rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>266.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.1474</td>\n",
              "      <td>(46.0/312.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>27.0</td>\n",
              "      <td>292.0</td>\n",
              "      <td>0.0846</td>\n",
              "      <td>(27.0/319.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Total</td>\n",
              "      <td>293.0</td>\n",
              "      <td>338.0</td>\n",
              "      <td>0.1157</td>\n",
              "      <td>(73.0/631.0)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             negative  positive   Error           Rate\n",
              "0  negative     266.0      46.0  0.1474   (46.0/312.0)\n",
              "1  positive      27.0     292.0  0.0846   (27.0/319.0)\n",
              "2     Total     293.0     338.0  0.1157   (73.0/631.0)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B3kRTSvHx8Aq",
        "outputId": "4b5c9d1d-e489-474e-af93-86897fd2dc08"
      },
      "source": [
        "# This outputs the model as a mojo file which is meant to be a binary object \r\n",
        "# that can work in other languages\r\n",
        "h2o_model.leader.download_mojo(path = \"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/H2O\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/CIT/FYP/ImplementationFiles/ExportedModels/H2O/XGBoost_2_AutoML_20210224_221844.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSCHYar_6dB3"
      },
      "source": [
        "\r\n",
        "# def main():\r\n",
        "#   # X_train, y_train, X_test, y_test = prepare_data()\r\n",
        "#   feature_vector, labels = prepare_data()\r\n",
        "\r\n",
        "#   # Convert feature_vectors into a pandas dataframe of \r\n",
        "#   # term frequency inverse document frequency of each word\r\n",
        "#   tfidf_tf = pd.DataFrame(feature_vector)\r\n",
        "  \r\n",
        "#   # Add the labels\r\n",
        "#   tfidf_tf['labels'] = labels\r\n",
        "\r\n",
        "#   #==========================================\r\n",
        "#   # Insert the code for running the libraries in here\r\n",
        "#   h2o_tfidf = h2o.H2OFrame(tfidf_tf)\r\n",
        "#   # print(h2o_tfidf.head())\r\n",
        "\r\n",
        "#   y = \"labels\"\r\n",
        "#   x = h2o_tfidf.columns\r\n",
        "#   x.remove(y)\r\n",
        "\r\n",
        "#   h2o_tfidf_train, h2o_tfidf_test, h2o_tfidf_valid = h2o_tfidf.split_frame(ratios=[0.7, 0.15])\r\n",
        " \r\n",
        "#   from h2o.automl import H2OAutoML\r\n",
        "#   aml = H2OAutoML(max_models = 10, seed = 10, exclude_algos = [\"StackedEnsemble\", \"DeepLearning\"], verbosity=\"info\", nfolds=0)\r\n",
        "  \r\n",
        "#   aml.train(x = x, y = y, training_frame = h2o_tfidf_train, validation_frame=h2o_tfidf_valid)\r\n",
        "\r\n",
        "#   lb = aml.leaderboard\r\n",
        "  \r\n",
        "#   lb.head()\r\n",
        "\r\n",
        "\r\n",
        "#   # Train the automl, x sets the training columns, y is the y columns\r\n",
        "#   # automodel.train(x = x, y = y, training_frame = imdb_train, validation_frame=imdb_valid)\r\n",
        "#   # aml = H2OAutoML(max_models=20, seed=1)\r\n",
        "  \r\n",
        "\r\n",
        "#   #==========================================\r\n",
        "\r\n",
        "# main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}