{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MLBox MainNotebook",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jdsNoOXJWNIsrtwL_ajhOY8N3M2M6VVi",
      "authorship_tag": "ABX9TyObN10DfM5Mh1OEEHMOUNnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CianOSull/AutoML_With_SA_FYP_2021/blob/MLBox/Copy_of_MLBox_MainNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_zrt8fk5AJt"
      },
      "source": [
        "# Generic Notebook for running all the libraries\n",
        "\n",
        "How this notebook works is that it contains the code\n",
        "for loading and cleaning the dataset.\n",
        "\n",
        "Then there is multiple branches created on the\n",
        "Github that include the code for running each library.\n",
        "\n",
        "E.g. MLBox branch has the code for running MLBox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwjBx4O_K8E"
      },
      "source": [
        "# CURRENT BRANCH: MLBox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew8tq1lj6v1L"
      },
      "source": [
        "# Install the necessary library\n",
        "Run the install code in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wFpyMaK6tMs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73a5e66c-e06b-449b-fa7c-2cca6cf5020b"
      },
      "source": [
        "# Insert any install comamnds in this cell\n",
        "!pip install mlbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mlbox\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/26/6236ca21e762067fbb7a6cd388fc9812380af88ae007ca42da9ef6384ed8/mlbox-0.8.5.tar.gz\n",
            "Collecting numpy==1.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/ce/d0b92f0283faa4da76ea82587ff9da70104e81f59ba14f76c87e4196254e/numpy-1.18.2-cp37-cp37m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from mlbox) (1.4.1)\n",
            "Collecting matplotlib==3.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/2a/e47bbd9396af32376863a426baed62d9bf3091f81defd1fe81c5f33b11a3/matplotlib-3.0.3-cp37-cp37m-manylinux1_x86_64.whl (13.0MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0MB 31.6MB/s \n",
            "\u001b[?25hCollecting hyperopt==0.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/11/8bbbb5edb78c40a2bd0f6b730e3dc0f29ffbaea9a59520eb9622951e9151/hyperopt-0.2.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 47.6MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 166kB/s \n",
            "\u001b[?25hCollecting joblib==0.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 4.7MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/db/7d8204ddba84ab5d1e4fd1af8f82bbe39c589488bee71e45c662f4144010/scikit_learn-0.22.1-cp37-cp37m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 35.5MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/5c/f1d66de5dde6f3ff528f6ea1fd0757a0e594d17debb3ec7f82daa967ea9a/tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 49kB/s \n",
            "\u001b[?25hCollecting lightgbm==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/9d/ddcb2f43aca194987f1a99e27edf41cf9bc39ea750c3371c2a62698c509a/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 28.4MB/s \n",
            "\u001b[?25hCollecting tables==3.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/5e/0463e9a2ae44838e36ca81dfe8a6e056b15cc15be77e7f8feae116f02ec7/tables-3.5.2-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 41.7MB/s \n",
            "\u001b[?25hCollecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->mlbox) (2.8.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox) (0.16.0)\n",
            "Collecting networkx==2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 28.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.2.3->mlbox) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->mlbox) (2018.9)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (0.36.2)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 38.9MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 35.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (1.32.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0->mlbox) (3.12.4)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables==3.5.2->mlbox) (2.7.3)\n",
            "Collecting mock>=2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.2->hyperopt==0.2.3->mlbox) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (54.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (0.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (1.28.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0->mlbox) (2.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->mlbox) (3.4.1)\n",
            "Building wheels for collected packages: mlbox, networkx, gast\n",
            "  Building wheel for mlbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mlbox: filename=mlbox-0.8.5-cp37-none-any.whl size=43754 sha256=6f9533d5e34664bf588ef312f376d83f3716ead07f2d68eba5b1ee20125f139d\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/71/0e/b72acfbcdaaf1e1480d9e7e50402b916093be47738a9d46ceb\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.2-py2.py3-none-any.whl size=1527322 sha256=35f2491c29d56130e73b85547dfd9b268503d189b3df20765a471db16524ea0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=5e86d6e040c51a83c998b503b150a7ade972870491687359f065cc5fe91f8692\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built mlbox networkx gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, matplotlib, networkx, hyperopt, pandas, joblib, scikit-learn, gast, tensorflow-estimator, tensorboard, keras-applications, tensorflow, lightgbm, mock, tables, xlrd, mlbox\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: networkx 2.5\n",
            "    Uninstalling networkx-2.5:\n",
            "      Successfully uninstalled networkx-2.5\n",
            "  Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed gast-0.2.2 hyperopt-0.2.3 joblib-0.14.1 keras-applications-1.0.8 lightgbm-2.3.1 matplotlib-3.0.3 mlbox-0.8.5 mock-4.0.3 networkx-2.2 numpy-1.18.2 pandas-0.25.3 scikit-learn-0.22.1 tables-3.5.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TNIZJ6c5hcF"
      },
      "source": [
        "# Preprocessing Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njEYGcZQ42t-"
      },
      "source": [
        "# Import the necessary modules for cleaning\n",
        "import math\n",
        "import time \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93NNPX-A5srS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cd7279-8724-4d82-ae5d-41b1b166de7a"
      },
      "source": [
        "# Download the necessary parts for the NLTK module\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHeKvyXg0y76"
      },
      "source": [
        "# Model Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK94AB74IZyF"
      },
      "source": [
        "# Example code:\n",
        "\n",
        "https://www.kaggle.com/axelderomblay/running-mlbox-auto-ml-package-on-titanic\n",
        "\n",
        "# Youtube code:\n",
        "https://www.youtube.com/watch?v=omd8SazsHaI&t=106s\n",
        "\n",
        "https://github.com/sergeiissaev/youtube/blob/master/mlbox.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSgIILvsDLl4"
      },
      "source": [
        "from mlbox.preprocessing import *\n",
        "from mlbox.optimisation import *\n",
        "from mlbox.prediction import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o67NG-4QC05Z"
      },
      "source": [
        "# From Docs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoNikR7BDsF1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# A dense numpy array of the sparse matrix outputted can be made by using:\n",
        "# X_train.toarray()\n",
        "def create_data(num, process):  \n",
        "  imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\n",
        "\n",
        "  # the next step is to randomize the rows of the data\n",
        "  imdb_df = imdb_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\n",
        "\n",
        "  features = imdb_df.review.values[0:num]\n",
        "  labels = imdb_df.sentiment.values[0:num]\n",
        "\n",
        "  no_samples = 0.8\n",
        "\n",
        "  # This gets the percentage of indexes from feature vector and uses those for training\n",
        "  train_txt = features[0:int(no_samples*len(features))]\n",
        "  y_train = labels[0:int(no_samples*len(labels))]\n",
        "\n",
        "  # Go from the index that was used for training to the final\n",
        "  test_txt = features[int(no_samples*len(features)):len(features)]\n",
        "  y_test = labels[int(no_samples*len(labels)):len(labels)]\n",
        "\n",
        "  # This cell has the sk learn functions\n",
        "  # Allows you to choose which function you want to use\n",
        "  if process == 0:\n",
        "    ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    ctv.fit(features)\n",
        "\n",
        "    X_train_sparse = ctv.transform(train_txt)\n",
        "    X_test_sparse = ctv.transform(test_txt)\n",
        "\n",
        "  else:\n",
        "    tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    tfv.fit(features)\n",
        "\n",
        "    X_train_sparse = tfv.transform(train_txt)\n",
        "    X_test_sparse = tfv.transform(test_txt)\n",
        "\n",
        "  # Converting to numpy arrays for more generic format\n",
        "  X_train = X_train_sparse.toarray()\n",
        "  X_test = X_test_sparse.toarray()\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi0QE6BVDyWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7abc54d-7e34-4f17-fc5b-8ac75afadaae"
      },
      "source": [
        "# Size = 10,000 failed\n",
        "# Size = 3,000 failed\n",
        "# Size 2,500 seems to be limit\n",
        "X_train, y_train, X_test, y_test = create_data(500, 1)\n",
        "\n",
        "# Dual Column dataframe way\n",
        "# # Convert feature_vectors into a pandas dataframe of \n",
        "# # term frequency inverse document frequency of each word\n",
        "# tfidf_train = pd.DataFrame(columns = ['features', 'labels'])\n",
        "\n",
        "# # This is a dataframe with each row having a list\n",
        "# for i in range(len(X_train)):\n",
        "#   tfidf_train.loc[i] = [X_train[i].tolist()] + [y_train[i]]\n",
        "\n",
        "# # Convert feature_vectors into a pandas dataframe of \n",
        "# # term frequency inverse document frequency of each word\n",
        "# tfidf_test = pd.DataFrame(columns = ['features'])\n",
        "\n",
        "# # This is a dataframe with each row having a list\n",
        "# for i in range(len(X_test)):\n",
        "#   tfidf_test.loc[i] = [X_test[i].tolist()]\n",
        "\n",
        "# =================================================================\n",
        "# Multiple column dataframe way\n",
        "tfidf_train = pd.DataFrame(X_train)\n",
        "tfidf_test =  pd.DataFrame(X_test)\n",
        "\n",
        "tfidf_train['labels'] = y_train\n",
        "tfidf_test['labels'] = y_test\n",
        "\n",
        "print(tfidf_train.head())\n",
        "print(tfidf_test.head())\n",
        "print(len(tfidf_train))\n",
        "print(len(tfidf_test))\n",
        "\n",
        "tfidf_train.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", index=False)\n",
        "tfidf_test.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          0    1    2    3    4  ...     13565  13566  13567  13568  labels\n",
            "0  0.000000  0.0  0.0  0.0  0.0  ...  0.000000    0.0    0.0    0.0       0\n",
            "1  0.000000  0.0  0.0  0.0  0.0  ...  0.000000    0.0    0.0    0.0       0\n",
            "2  0.044442  0.0  0.0  0.0  0.0  ...  0.000000    0.0    0.0    0.0       1\n",
            "3  0.000000  0.0  0.0  0.0  0.0  ...  0.287005    0.0    0.0    0.0       1\n",
            "4  0.000000  0.0  0.0  0.0  0.0  ...  0.000000    0.0    0.0    0.0       1\n",
            "\n",
            "[5 rows x 13570 columns]\n",
            "     0    1    2    3    4    5  ...  13564  13565  13566  13567  13568  labels\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       0\n",
            "1  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       1\n",
            "2  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       1\n",
            "3  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       0\n",
            "4  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0    0.0    0.0    0.0       1\n",
            "\n",
            "[5 rows x 13570 columns]\n",
            "400\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J0YJKJ-fLbf",
        "outputId": "5ae94e06-e720-4061-c52e-ec677f3973d0"
      },
      "source": [
        "# Size = 10,000 failed\n",
        "# Size = 3,000 failed\n",
        "# Size 2,500 seems to be limit\n",
        "X_train, y_train, X_test, y_test = create_data(500, 1)\n",
        "\n",
        "# Dual Column dataframe way\n",
        "# Convert feature_vectors into a pandas dataframe of \n",
        "# term frequency inverse document frequency of each word\n",
        "tfidf_train = pd.DataFrame(columns = ['features', 'labels'])\n",
        "\n",
        "# This is a dataframe with each row having a list\n",
        "for i in range(len(X_train)):\n",
        "  tfidf_train.loc[i] = [X_train[i].tolist()] + [y_train[i]]\n",
        "\n",
        "# Convert feature_vectors into a pandas dataframe of \n",
        "# term frequency inverse document frequency of each word\n",
        "tfidf_test = pd.DataFrame(columns = ['features'])\n",
        "\n",
        "# This is a dataframe with each row having a list\n",
        "for i in range(len(X_test)):\n",
        "  tfidf_test.loc[i] = [X_test[i].tolist()]\n",
        "\n",
        "print(tfidf_train.head())\n",
        "print(tfidf_test.head())\n",
        "print(len(tfidf_train))\n",
        "print(len(tfidf_test))\n",
        "\n",
        "# tfidf_train.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", index=False)\n",
        "# tfidf_test.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            features labels\n",
            "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      1\n",
            "1  [0.13991044080497225, 0.0, 0.0, 0.0, 0.0, 0.0,...      0\n",
            "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      1\n",
            "3  [0.04381093567293872, 0.0, 0.0, 0.0, 0.0, 0.0,...      1\n",
            "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0668575653297...      0\n",
            "                                            features\n",
            "0  [0.09827054713613577, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
            "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0693357444139...\n",
            "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0817385506729...\n",
            "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0434669084241377, ...\n",
            "4  [0.04637218434805809, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
            "400\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3kKfFI6C3H2"
      },
      "source": [
        "paths = [\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", \"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\"]\n",
        "target_name = \"labels\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDEulXWRE7aq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40d40c8-c385-413c-8701-aadb5ddbb5a6"
      },
      "source": [
        "data = Reader(sep=\",\").train_test_split(paths, target_name)\n",
        "# data = Drift_thresholder().fit_transform(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "reading csv : tfidf_train.csv ...\n",
            "cleaning data ...\n",
            "CPU time: 35.53465175628662 seconds\n",
            "\n",
            "reading csv : tfidf_test.csv ...\n",
            "cleaning data ...\n",
            "CPU time: 27.13295078277588 seconds\n",
            "\n",
            "You have no test dataset !\n",
            "\n",
            "> Number of common features : 13569\n",
            "\n",
            "gathering and crunching for train and test datasets ...\n",
            "reindexing for train and test datasets ...\n",
            "dropping training duplicates ...\n",
            "dropping constant variables on training set ...\n",
            "\n",
            "> Number of categorical features: 0\n",
            "> Number of numerical features: 13569\n",
            "> Number of training samples : 500\n",
            "> Number of test samples : 0\n",
            "\n",
            "> You have no missing values on train set...\n",
            "\n",
            "> Task : classification\n",
            "0.0    258\n",
            "1.0    242\n",
            "Name: labels, dtype: int64\n",
            "\n",
            "encoding target ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ONbkf0NFRHk"
      },
      "source": [
        "\n",
        "# Basic with params optimise possible pipeline\n",
        "---\n",
        "\n",
        "# BIG NOTE\n",
        "Dual Column dataframe works.\n",
        "\n",
        "But predictor doesn't. Look into a way of fixing this this, one idea is maybe copy the parameters found and  recreate it myself by setting the paramters to be the space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB8wi5DHFKYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e28f23-e0ef-4b86-80c8-12e2a413134c"
      },
      "source": [
        "# example space\n",
        "space = {\n",
        "\n",
        "        'ne__numerical_strategy' : {\"space\" : [0, 'mean']},\n",
        "\n",
        "        'ce__strategy' : {\"space\" : [\"label_encoding\", \"random_projection\", \"entity_embedding\"]},\n",
        "\n",
        "        'fs__strategy' : {\"space\" : [\"variance\", \"rf_feature_importance\"]},\n",
        "        'fs__threshold': {\"search\" : \"choice\", \"space\" : [0.1, 0.2, 0.3]},\n",
        "\n",
        "        'est__strategy' : {\"space\" : [\"LightGBM\"]},\n",
        "        'est__max_depth' : {\"search\" : \"choice\", \"space\" : [5,6]},\n",
        "        'est__subsample' : {\"search\" : \"uniform\", \"space\" : [0.6,0.9]}\n",
        "\n",
        "        }\n",
        "\n",
        "# Titanic example\n",
        "# space = {\n",
        "    \n",
        "#         'est__strategy':{\"search\":\"choice\",\n",
        "#                                   \"space\":[\"LightGBM\"]},    \n",
        "#         'est__n_estimators':{\"search\":\"choice\",\n",
        "#                                   \"space\":[150]},    \n",
        "#         'est__colsample_bytree':{\"search\":\"uniform\",\n",
        "#                                   \"space\":[0.8,0.95]},\n",
        "#         'est__subsample':{\"search\":\"uniform\",\n",
        "#                                   \"space\":[0.8,0.95]},\n",
        "#         'est__max_depth':{\"search\":\"choice\",\n",
        "#                                   \"space\":[5,6,7,8,9]},\n",
        "#         'est__learning_rate':{\"search\":\"choice\",\n",
        "#                                   \"space\":[0.07]} \n",
        "    \n",
        "#         }\n",
        "\n",
        "# Example found on kaggle\n",
        "# space = {\n",
        "    \n",
        "#         'est__strategy':{\"search\":\"choice\",\n",
        "#                                   \"space\":[\"LightGBM\"]},    \n",
        "#         'est__n_estimators':{\"search\":\"choice\",\n",
        "#                                   \"space\":[700]},    \n",
        "#         'est__colsample_bytree':{\"search\":\"uniform\",\n",
        "#                                   \"space\":[0.77,0.82]},\n",
        "#         'est__subsample':{\"search\":\"uniform\",\n",
        "#                                   \"space\":[0.73,0.8]},\n",
        "#         'est__max_depth':{\"search\":\"choice\",\n",
        "#                                   \"space\":[5,6,7]},\n",
        "#         'est__learning_rate':{\"search\":\"uniform\",\n",
        "#                                   \"space\":[0.008, 0.02]} \n",
        "    \n",
        "#         }\n",
        "\n",
        "\n",
        "opt = Optimiser(scoring = \"accuracy\", n_folds = 5)\n",
        "\n",
        "best = opt.optimise(space, data, max_evals = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'label_encoding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'rf_feature_importance', 'threshold': 0.3}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 5, 'subsample': 0.7525603611553028, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/optimisation/optimiser.py:74: UserWarning: Optimiser will save all your fitted models into directory 'save/joblib'. Please clear it regularly.\n",
            "  +str(self.to_path)+\"/joblib'. Please clear it regularly.\")\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.50s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.50s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = 0.7220000000000001\n",
            "VARIANCE : 0.059464274989273994 (fold 1 = 0.66, fold 2 = 0.68, fold 3 = 0.82, fold 4 = 0.69, fold 5 = 0.76)\n",
            "CPU time: 26.13408851623535 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 'mean', 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'variance', 'threshold': 0.2}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 6, 'subsample': 0.6215741815476972, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            "MEAN SCORE : accuracy = 0.722\n",
            "VARIANCE : 0.04874423042781576 (fold 1 = 0.68, fold 2 = 0.67, fold 3 = 0.79, fold 4 = 0.7, fold 5 = 0.77)\n",
            "CPU time: 7.5632781982421875 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 'mean', 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'entity_embedding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'variance', 'threshold': 0.1}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 6, 'subsample': 0.6120004733986184, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            " 40%|████      | 2/5 [00:33<01:01, 20.58s/trial, best loss: -0.7220000000000001]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.50s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MEAN SCORE : accuracy = 0.722\n",
            "VARIANCE : 0.04874423042781576 (fold 1 = 0.68, fold 2 = 0.67, fold 3 = 0.79, fold 4 = 0.7, fold 5 = 0.77)\n",
            "CPU time: 25.007959127426147 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'entity_embedding'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'variance', 'threshold': 0.3}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 5, 'subsample': 0.8964094191540564, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            "MEAN SCORE : accuracy = 0.7220000000000001\n",
            "VARIANCE : 0.059464274989273994 (fold 1 = 0.66, fold 2 = 0.68, fold 3 = 0.82, fold 4 = 0.69, fold 5 = 0.76)\n",
            "CPU time: 14.379096269607544 seconds\n",
            "##################################################### testing hyper-parameters... #####################################################\n",
            ">>> NA ENCODER :{'numerical_strategy': 0, 'categorical_strategy': '<NULL>'}\n",
            ">>> CA ENCODER :{'strategy': 'random_projection'}\n",
            ">>> FEATURE SELECTOR :{'strategy': 'variance', 'threshold': 0.1}\n",
            ">>> ESTIMATOR :{'strategy': 'LightGBM', 'max_depth': 6, 'subsample': 0.7101975013553626, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8, 'importance_type': 'split', 'learning_rate': 0.05, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': True, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'nthread': -1, 'seed': 0}\n",
            "MEAN SCORE : accuracy = 0.722\n",
            "VARIANCE : 0.04874423042781576 (fold 1 = 0.68, fold 2 = 0.67, fold 3 = 0.79, fold 4 = 0.7, fold 5 = 0.77)\n",
            "CPU time: 7.293528079986572 seconds\n",
            "100%|██████████| 5/5 [01:20<00:00, 16.10s/trial, best loss: -0.7220000000000001]\n",
            "\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BEST HYPER-PARAMETERS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "{'ce__strategy': 'label_encoding', 'est__max_depth': 5, 'est__strategy': 'LightGBM', 'est__subsample': 0.7525603611553028, 'fs__strategy': 'rf_feature_importance', 'fs__threshold': 0.3, 'ne__numerical_strategy': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESVzj8vjDrjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dc0bacb-fcf6-4d0d-ba73-6ea783c956b0"
      },
      "source": [
        "Predictor().fit_predict(best, data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "fitting the pipeline ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.53s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py:315: UserWarning: Persisting input arguments took 0.56s to run.\n",
            "If this happens often in your code, it can cause performance problems \n",
            "(results will be correct in all cases). \n",
            "The reason for this is probably some large input arguments for a wrapped\n",
            " function (e.g. large strings).\n",
            "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
            " example so that they can fix the problem.\n",
            "  **fit_params_steps[name])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU time: 6.131330251693726 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlbox/prediction/predictor.py:392: UserWarning: Unable to get feature importances !\n",
            "  warnings.warn(\"Unable to get feature importances !\")\n",
            "/usr/local/lib/python3.7/dist-packages/mlbox/prediction/predictor.py:405: UserWarning: You have no test dataset. Cannot predict !\n",
            "  warnings.warn(\"You have no test dataset. Cannot predict !\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<mlbox.prediction.predictor.Predictor at 0x7f7e29bdb410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "text": [
            "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x7f7e614badd0> (for post_execute):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpost_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mpost_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mdraw_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# IPython >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/_pylab_helpers.py\u001b[0m in \u001b[0;36mdraw_all\u001b[0;34m(cls, force)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf_mgr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mf_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1899\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mDraw\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfigure\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \"\"\"\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;31m# acquire a lock on the shared font cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_new_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Image size of 1440x293112 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m                         \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2049\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2050\u001b[0m                     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \"\"\"\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mDraw\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfigure\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \"\"\"\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;31m# acquire a lock on the shared font cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_new_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Image size of 1440x293112 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x293112 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTcmR1T_I__m"
      },
      "source": [
        "# How to get predicitons:\n",
        "\n",
        "Predictions are saved to folder called save so we need to read them in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGmpgLkRHadq"
      },
      "source": [
        "preds = pd.read_csv(\"save/labels_predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWwpvQaCI0jb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910ef563-ff50-409d-e062-b2a3d5a4383c"
      },
      "source": [
        "print(preds[\"labels_predicted\"].values)\n",
        "y_pred = preds[\"labels_predicted\"].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7KEzd1JJGQm"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKDb3Gu2C2cp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_FzH-Ejrb_y"
      },
      "source": [
        "# OLD CODE (IGNORE)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sPOCEGS5Xgn"
      },
      "source": [
        "# Create the set of stopwords for cleaning text\n",
        "stopwords = set(w.rstrip() for w in open('/content/drive/MyDrive/CIT/FYP/ImplementationFiles/stopwords.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ptneBc5tAN"
      },
      "source": [
        "# This funciton handles celaning text\n",
        "def clean_text(text):\n",
        "    # Create the lemmatizer\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    # Get rid of non alpha characters except \"'\" as it is needed for the lemment\n",
        "    text = \"\".join(c for c in text if c.isalnum() or c == \" \" or \"'\")\n",
        "    \n",
        "    # Get rid of capitals\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Tokenize the words    \n",
        "    # Create tokens of each word\n",
        "    token_text = word_tokenize(text)\n",
        "    \n",
        "    # Get rid of any piece of text that isn't over 2 characters\n",
        "    token_text = [t for t in token_text if len(t) > 2] \n",
        "    \n",
        "    # Put words in base form by doing lemmatization\n",
        "    token_text = [wordnet_lemmatizer.lemmatize(t) for t in token_text]\n",
        "\n",
        "    # Remove stopwords\n",
        "    token_text = [t for t in token_text if t not in stopwords]\n",
        "    \n",
        "    # Return the tokens\n",
        "    return token_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu5dFDM75tjR"
      },
      "source": [
        "# This function will get the term frequencies for word in the review\n",
        "# TF = Term I frequency in document/total words in document\n",
        "def calc_tf(term_count, review_corpus):\n",
        "    # A dictionary of all the term frequencies found\n",
        "    tf_freq = dict.fromkeys(term_count.keys(), 0)   \n",
        "    \n",
        "    # Review corpus is a tokenized list so the total words iteh length\n",
        "    total_words = len(review_corpus)\n",
        "    \n",
        "    # Calculate the term frequency for each word\n",
        "    for word, count in term_count.items():\n",
        "        tf_freq[word] = count/total_words\n",
        "        \n",
        "    return tf_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i64Bq0rH5uN0"
      },
      "source": [
        "# This calcualtes the idf\n",
        "# IDF = log(2)*(Total number of Documents/documents frequency or documents with term)\n",
        "def calc_idf(unique_terms, list_doc_terms):   \n",
        "    # A dicitonary of all the inverse document frequencies\n",
        "    idf = dict.fromkeys(unique_terms, 0)\n",
        "    \n",
        "    # Basically list_doc_terms has all the documents with the term count for each word\n",
        "    # You go through each document count the terms where they occured\n",
        "    for doc_terms in list_doc_terms:  \n",
        "        # This for loop is counting the amount of document a word was in\n",
        "        for word, value in doc_terms.items():\n",
        "            if 0 < value:\n",
        "                idf[word] += 1\n",
        "        \n",
        "    # Now we calculate idf\n",
        "    for word, value in idf.items():\n",
        "        idf[word] = math.log10(10 / float(value))\n",
        "    \n",
        "    return idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XTmL87F5uaU"
      },
      "source": [
        "# Modified this function to return a list as dictionaries arn't needed anymore\n",
        "def calc_tf_idf(tf, idf, n_terms):\n",
        "    # Create an array that is of length of the number of unique terms\n",
        "    tf_idf_array = np.zeros(n_terms)\n",
        "    \n",
        "    for index, (word, value) in enumerate(tf.items()):\n",
        "        # Add the tfidf to the array\n",
        "        tf_idf_array[index] = value*idf[word]\n",
        "    \n",
        "    return tf_idf_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzOme9px5uks"
      },
      "source": [
        "def process_text(text_data):\n",
        "    # A list of all the cleaned reviews\n",
        "    doc_list = []\n",
        "    \n",
        "    # List of all the unique terms\n",
        "    unique_terms = []\n",
        "    \n",
        "    # A list of all the term frequencies\n",
        "    tf_list = []\n",
        "    \n",
        "    for review in text_data:\n",
        "        # First clean the review\n",
        "        clean_review = clean_text(review)\n",
        "        \n",
        "        # Keeps track of the term counts for each word\n",
        "        count_dict = {}\n",
        "        \n",
        "        # Now lets find the total count for each word\n",
        "        for token in clean_review:\n",
        "            if token not in count_dict:\n",
        "                count_dict[token] = 1\n",
        "            else:\n",
        "                count_dict[token] += 1\n",
        "        \n",
        "        # Caclulate the term frequencies for each document\n",
        "        tf_list.append(calc_tf(count_dict, clean_review))\n",
        "        \n",
        "        # Then add the dictionary of counts for each document to the list\n",
        "        doc_list.append(count_dict)\n",
        "        \n",
        "        # Then add the new unique terms\n",
        "        unique_terms = set(unique_terms).union(set(clean_review))\n",
        "    \n",
        "    # Calculate the inverse document frequency value\n",
        "    idf = calc_idf(unique_terms, doc_list)\n",
        "    \n",
        "    # This array will contain the tfidf values for each term in each review\n",
        "    tfidf_values = np.zeros((len(tf_list), len(unique_terms)))\n",
        "    \n",
        "    # Now we can get the TFIDF for each document\n",
        "    for index, term_freq in enumerate(tf_list):\n",
        "        # This will return an array of the tfidf values calculated.\n",
        "        # The length of the unique terms list is passed in so that the \n",
        "        # Array that is returned matches the tfidf array\n",
        "        tf_idf_array = calc_tf_idf(term_freq, idf, len(unique_terms))\n",
        "        # Add this to the overall tfidf values calculated\n",
        "        tfidf_values[index,:] = tf_idf_array\n",
        "    \n",
        "    return tfidf_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj8BBqqCfTlt"
      },
      "source": [
        "# Prepare the data\n",
        "def prepare_data(num):\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load the dataset\n",
        "    # imdb_df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "    imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\n",
        "    print(\"Dataset loaded\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Change each positive and negative value to 1 and 0 respectively    \n",
        "    imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\n",
        "    \n",
        "    # # For testing, a much smaller dataset is going to be used\n",
        "    # imdb_df = imdb_df.head(5000)\n",
        "\n",
        "    # Group all the negative reviews and get the first 2500\n",
        "    imdb_df_neg = (imdb_df[imdb_df['sentiment'] == 0])[0:num]\n",
        "    # imdb_df_neg = (imdb_df[imdb_df['sentiment'] == \"negative\"])[0:num]\n",
        "    \n",
        "    # Group all the positive and get the first 2500\n",
        "    imdb_df_pos = imdb_df[imdb_df['sentiment'] == 1][0:num]\n",
        "    # imdb_df_pos = imdb_df[imdb_df['sentiment'] == \"positive\"][0:num]\n",
        "    \n",
        "    test_df = pd.concat([imdb_df_neg, imdb_df_pos]) \n",
        "    # print(test_df)\n",
        "    \n",
        "    # .values on a column of a dataframe returns a numpy array\n",
        "    # This is a numpy array of all the reviews\n",
        "    # initial_reviews = imdb_df['review'].values\n",
        "    initial_reviews = test_df['review'].values\n",
        "    \n",
        "    print(\"Creating Feature Vector\")\n",
        "    print(\"=\"*50)\n",
        "    start = time.time()\n",
        "    # Process the text data and create teh feature vector\n",
        "    feature_vector = process_text(initial_reviews)\n",
        "    end = time.time()\n",
        "    print(\"Feature Vector Created\")\n",
        "    print(len(feature_vector))\n",
        "    print(f\"Execution time is {end - start} secs\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # This is a numpy array of all the positive and negativelabels\n",
        "    # labels = imdb_df['sentiment'].values\n",
        "    labels = test_df['sentiment'].values\n",
        "    \n",
        "    # Shuffle the labesl and feature vector using sklearn shuffle\n",
        "    feature_vector, labels = shuffle(feature_vector, labels)\n",
        "    \n",
        "    # Creating train and test data\n",
        "    # The splits will be 80:20 \n",
        "    no_samples = 0.8\n",
        "    \n",
        "    # This gets the percentage of indexes from feature vector and uses those for training\n",
        "    X_train = feature_vector[0:int(no_samples*len(feature_vector))]\n",
        "    y_train = labels[0:int(no_samples*len(labels))]\n",
        "    \n",
        "    # Go from the index that was used for training to the final\n",
        "    X_test = feature_vector[int(no_samples*len(feature_vector)):len(feature_vector)]\n",
        "    y_test = labels[int(no_samples*len(labels)):len(labels)]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "    # return feature_vector, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsjFoaemrm4U"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eSjfvAjipKC"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# A dense numpy array of the sparse matrix outputted can be made by using:\n",
        "# X_train.toarray()\n",
        "def create_data(num, process):  \n",
        "  imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\n",
        "\n",
        "  # the next step is to randomize the rows of the data\n",
        "  imdb_df = imdb_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\n",
        "\n",
        "  features = imdb_df.review.values[0:num]\n",
        "  labels = imdb_df.sentiment.values[0:num]\n",
        "\n",
        "  no_samples = 0.8\n",
        "\n",
        "  # This gets the percentage of indexes from feature vector and uses those for training\n",
        "  train_txt = features[0:int(no_samples*len(features))]\n",
        "  y_train = labels[0:int(no_samples*len(labels))]\n",
        "\n",
        "  # Go from the index that was used for training to the final\n",
        "  test_txt = features[int(no_samples*len(features)):len(features)]\n",
        "  y_test = labels[int(no_samples*len(labels)):len(labels)]\n",
        "\n",
        "  # This cell has the sk learn functions\n",
        "  # Allows you to choose which function you want to use\n",
        "  if process == 0:\n",
        "    ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    ctv.fit(features)\n",
        "\n",
        "    X_train_sparse = ctv.transform(train_txt)\n",
        "    X_test_sparse = ctv.transform(test_txt)\n",
        "\n",
        "  else:\n",
        "    tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
        "\n",
        "    tfv.fit(features)\n",
        "\n",
        "    X_train_sparse = tfv.transform(train_txt)\n",
        "    X_test_sparse = tfv.transform(test_txt)\n",
        "\n",
        "  # Converting to numpy arrays for more generic format\n",
        "  X_train = X_train_sparse.toarray()\n",
        "  X_test = X_test_sparse.toarray()\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guNpJYYeiyG6"
      },
      "source": [
        "# Size = 10,000 failed\n",
        "# Size = 3,000 failed\n",
        "# Size 2,500 seems to be limit\n",
        "X_train, y_train, X_test, y_test = create_data(2500, 1)\n",
        "\n",
        "# Convert feature_vectors into a pandas dataframe of \n",
        "# term frequency inverse document frequency of each word\n",
        "tfidf_train = pd.DataFrame(columns = ['features', 'labels'])\n",
        "\n",
        "# This is a dataframe with each row having a list\n",
        "for i in range(len(X_train)):\n",
        "  tfidf_train.loc[i] = [X_train[i].tolist()] + [y_train[i]]\n",
        "\n",
        "# Convert feature_vectors into a pandas dataframe of \n",
        "# term frequency inverse document frequency of each word\n",
        "tfidf_test = pd.DataFrame(columns = ['features'])\n",
        "\n",
        "# This is a dataframe with each row having a list\n",
        "for i in range(len(X_test)):\n",
        "  tfidf_test.loc[i] = [X_test[i].tolist()]\n",
        "\n",
        "print(tfidf_train.head())\n",
        "print(tfidf_test.head())\n",
        "print(len(tfidf_train))\n",
        "print(len(tfidf_test))\n",
        "\n",
        "tfidf_train.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", index=False)\n",
        "tfidf_test.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dFVfv7piniV"
      },
      "source": [
        "# Old Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExawzZCGUJb3"
      },
      "source": [
        "X_train, y_train, X_test, y_test = prepare_data(500)\n",
        "\n",
        "# Convert feature_vectors into a pandas dataframe of \n",
        "# term frequency inverse document frequency of each word\n",
        "tfidf_train = pd.DataFrame(columns = ['features', 'labels'])\n",
        "\n",
        "# This is a dataframe with each row having a list\n",
        "for i in range(len(X_train)):\n",
        "  tfidf_train.loc[i] = [X_train[i].tolist()] + [y_train[i]]\n",
        "\n",
        "# Convert feature_vectors into a pandas dataframe of \n",
        "# term frequency inverse document frequency of each word\n",
        "tfidf_test = pd.DataFrame(columns = ['features'])\n",
        "\n",
        "# This is a dataframe with each row having a list\n",
        "for i in range(len(X_test)):\n",
        "  tfidf_test.loc[i] = [X_test[i].tolist()]\n",
        "\n",
        "print(tfidf_train.head())\n",
        "print(tfidf_test.head())\n",
        "print(len(tfidf_train))\n",
        "print(len(tfidf_test))\n",
        "\n",
        "tfidf_train.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", index=False)\n",
        "tfidf_test.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fanA4Rn83bz2"
      },
      "source": [
        "paths = [\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", \"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\"]\n",
        "target_name = \"labels\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6eEFJ_wdtyV"
      },
      "source": [
        "# Create model based on Titanic example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Tw618Y4mPT"
      },
      "source": [
        "# Read and clean files\n",
        "rd = Reader(sep = \",\")\n",
        "df = rd.train_test_split(paths, target_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL48b2ZkIGaK"
      },
      "source": [
        "# This can remove non stable features but probably shouldn't run at all for now\n",
        "# dft = Drift_thresholder()\n",
        "# df = dft.fit_transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qHyFlz3IQmM"
      },
      "source": [
        "opt = Optimiser(scoring = \"accuracy\", n_folds = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWAr0ZbrbeUP"
      },
      "source": [
        "space = {\n",
        "    \n",
        "        'est__strategy':{\"search\":\"choice\",\n",
        "                                  \"space\":[\"LightGBM\"]},    \n",
        "        'est__n_estimators':{\"search\":\"choice\",\n",
        "                                  \"space\":[150]},    \n",
        "        'est__colsample_bytree':{\"search\":\"uniform\",\n",
        "                                  \"space\":[0.8,0.95]},\n",
        "        'est__subsample':{\"search\":\"uniform\",\n",
        "                                  \"space\":[0.8,0.95]},\n",
        "        'est__max_depth':{\"search\":\"choice\",\n",
        "                                  \"space\":[5,6,7,8,9]},\n",
        "        'est__learning_rate':{\"search\":\"choice\",\n",
        "                                  \"space\":[0.07]} \n",
        "    \n",
        "        }\n",
        "\n",
        "print(\"Creating Fit the model\")\n",
        "print(\"=\"*50)\n",
        "start = time.time()\n",
        "params = opt.optimise(space, df,15)\n",
        "end = time.time()\n",
        "print(\"=\"*50)\n",
        "print(\"Model Fitting Finished\")\n",
        "print(f\"Execution time is {end - start} secs\")\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njs1QUnWcfpY"
      },
      "source": [
        "prd = Predictor()\n",
        "prd.fit_predict(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R57T5z6W-kOK"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = predict_output_vals\n",
        "testing_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy score {0}\".format(testing_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdr_J2jdGSgQ"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# paramters are y_true, y_pred\n",
        "C = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# lists for the confusion matrix\n",
        "true_positive = []\n",
        "true_negative = []\n",
        "false_postiive = []\n",
        "false_negatives = []\n",
        "\n",
        "true_positive.append(C[0,0])\n",
        "true_negative.append(C[1,1])            \n",
        "false_postiive.append(C[1,0])\n",
        "false_negatives.append(C[0,1])\n",
        "\n",
        "print(C)\n",
        "\n",
        "print((\"=\"*50))\n",
        "\n",
        "print(\"True positives:\", round(np.sum(true_positive)/len(y_test), 5), \"%\")\n",
        "print(\"True negatives:\", round(np.sum(true_negative)/len(y_test), 5), \"%\")\n",
        "print(\"False positives:\", round(np.sum(false_postiive)/len(y_test), 5), \"%\")\n",
        "print(\"False negatives:\", round(np.sum(false_negatives)/len(y_test), 5), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw7BWoS4Dr_h"
      },
      "source": [
        "true_positive = C[0,0]\n",
        "true_negative = C[1,1]            \n",
        "false_postiive = C[1,0]\n",
        "false_negatives = C[0,1]\n",
        "\n",
        "precision = true_positive/(true_positive+false_postiive)\n",
        "recall = true_positive/(true_positive/false_negatives)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hny7EopDuPt"
      },
      "source": [
        "f1_score = (precision*recall)/(precision+recall)\n",
        "\n",
        "print(f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA36OK4a3PHM"
      },
      "source": [
        "fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred)\n",
        "\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "print(\"AUC:\", auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeKomxM9CBZ8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %.2f)' %auc)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')\n",
        "plt.title('ROC curve MLBox')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RvvVN6NclD"
      },
      "source": [
        "# ===================================================\n",
        "\n",
        "# Old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJnTVLx6XBD"
      },
      "source": [
        "predict_output = pd.read_csv(\"save/labels_predictions.csv\")\n",
        "print(predict_output.head())\n",
        "# print(predict_output[\"labels_predicted\"].values)\n",
        "predict_output_vals = (predict_output[\"labels_predicted\"].values)\n",
        "print(predict_output_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G0RTyLSpxZf"
      },
      "source": [
        "# =============================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUDz9_E2fSCE"
      },
      "source": [
        "# Text data values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGjWVspVfhnR"
      },
      "source": [
        "X_train, y_train, X_test, y_test = prepare_data(1000)\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "print(len(X_test))\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yeMi9z4dp-U"
      },
      "source": [
        "# MLBOX EXample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAiMJ8aIfr6e"
      },
      "source": [
        "# This is a dataframe with a column for each value\n",
        "tfidf_train = pd.DataFrame(X_train)\n",
        "\n",
        "# Add the labels\n",
        "tfidf_train['labels'] = y_train\n",
        "\n",
        "# This is a dataframe with a column for each value\n",
        "tfidf_test = pd.DataFrame(X_test)\n",
        "\n",
        "# # Add the labels\n",
        "# tfidf_test['labels'] = y_test\n",
        "\n",
        "print(tfidf_train.head())\n",
        "print(tfidf_test.head())\n",
        "\n",
        "tfidf_train.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", index=False)\n",
        "tfidf_test.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrZ3YIef07Si"
      },
      "source": [
        "# This is a dataframe with a column for each value\n",
        "tfidf_train = pd.DataFrame(X_train)\n",
        "\n",
        "# Add the labels\n",
        "tfidf_train['labels'] = y_train\n",
        "\n",
        "# This is a dataframe with a column for each value\n",
        "tfidf_test = pd.DataFrame(X_test)\n",
        "\n",
        "# # Add the labels\n",
        "# tfidf_test['labels'] = y_test\n",
        "\n",
        "print(tfidf_train.head())\n",
        "print(tfidf_test.head())\n",
        "\n",
        "tfidf_train.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_train.csv\", index=False)\n",
        "tfidf_test.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI4EbKl2dXa8"
      },
      "source": [
        "data = Reader(sep=\",\").train_test_split(paths, target_name)  \n",
        "# data = Drift_thresholder().fit_transform(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY_XfW7_dxT0"
      },
      "source": [
        "opt = Optimiser().evaluate(None, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpFKieVnd0Cr"
      },
      "source": [
        "space = {\n",
        "\n",
        "        'ne__numerical_strategy' : {\"space\" : [0, 'mean']},\n",
        "\n",
        "        'ce__strategy' : {\"space\" : [\"label_encoding\", \"random_projection\", \"entity_embedding\"]},\n",
        "\n",
        "        'fs__strategy' : {\"space\" : [\"variance\", \"rf_feature_importance\"]},\n",
        "        'fs__threshold': {\"search\" : \"choice\", \"space\" : [0.1, 0.2, 0.3]},\n",
        "\n",
        "        'est__strategy' : {\"space\" : [\"LightGBM\"]},\n",
        "        'est__max_depth' : {\"search\" : \"choice\", \"space\" : [5,6]},\n",
        "        'est__subsample' : {\"search\" : \"uniform\", \"space\" : [0.6,0.9]}\n",
        "\n",
        "        }\n",
        "\n",
        "params = opt.optimise(space, df, 15)\n",
        "bestsd = opt.optimise(space, data, max_evals = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqmHRx8xd54b"
      },
      "source": [
        "Predictor().fit_predict(best, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q12yraWZz7xT"
      },
      "source": [
        "# Output Pandsa Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMieWz9Y0cKm"
      },
      "source": [
        "# Importing data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "feature_vector, labels = prepare_data(500)\n",
        "\n",
        "# Convert feature_vectors into a pandas dataframe of \n",
        "# term frequency inverse document frequency of each word\n",
        "# tfidf_tf = pd.DataFrame(feature_vector)\n",
        "tfidf_tf_2col = pd.DataFrame(columns = ['features', 'labels'])\n",
        "\n",
        "# This is a dataframe with each row having a list\n",
        "for i in range(len(feature_vector)):\n",
        "  tfidf_tf_2col.loc[i] = [feature_vector[i].tolist()] + [labels[i]]\n",
        "\n",
        "# This is a dataframe with a column for each value\n",
        "tfidf_tf_mult = pd.DataFrame(feature_vector)\n",
        "\n",
        "# Add the labels\n",
        "tfidf_tf_mult['labels'] = labels\n",
        "\n",
        "#=========================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbgWfwZm0hVv"
      },
      "source": [
        "print(tfidf_tf.head())\n",
        "tfidf_tf.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_2col.csv\", index=False)\n",
        "\n",
        "print(tfidf_tf_mult.head())\n",
        "tfidf_tf_mult.to_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_tf_mult.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wb6JUcZ0h9H"
      },
      "source": [
        "load_ouput_csv_2col = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_2col.csv\")\n",
        "\n",
        "print(load_ouput_csv_2col.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG6Y-Raz0mF5"
      },
      "source": [
        "load_ouput_csv_mult = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/tfidf_tf_mult.csv\")\n",
        "print(load_ouput_csv_mult.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJ2EkrfpHDB"
      },
      "source": [
        "print(\"Keep VM\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}