{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestingNotebook",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jYdoRQjW-Qv0-c9MW55ofev-zOnRdmql",
      "authorship_tag": "ABX9TyOZT6xQzFkNY3Xdo23xFbhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CianOSull/AutoML_With_SA_FYP_2021/blob/Preprocessing/TestingNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUbgZSDTCI5e"
      },
      "source": [
        "# Test Running Auto Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBNruLBQCTbZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgA3Wxfc4s10"
      },
      "source": [
        "# Text Preprocessing Code\r\n",
        "\r\n",
        "This area is for running the code that will process the text into an inputtable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JnWId_KvbqG",
        "outputId": "ae09b359-d948-40d8-cdb8-9e42ab8bba02"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\"\"\"\r\n",
        "Created on Thu Feb  4 10:54:03 2021\r\n",
        "\r\n",
        "@author: Cian\r\n",
        "\"\"\"\r\n",
        "#-------------------------------------------------\r\n",
        "#           ABOUT THIS FILE\r\n",
        "# This file is dedicated to testing the functionality of NLTK\r\n",
        "# \r\n",
        "# Useful links:\r\n",
        "# https://www.nltk.org/\r\n",
        "# https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\r\n",
        "# https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92#:~:text=The%20easiest%20way%20to%20upload%20a%20CSV%20file,below%20(a%20cleaner%20method%20but%20it%E2%80%99s%20not%20necessary).\r\n",
        "# https://medium.com/python-in-plain-english/implementing-your-first-xgboost-model-with-scikit-learn-761e2b6cfcf8\r\n",
        "#\r\n",
        "#-------------------------------------------------\r\n",
        "\r\n",
        "import math\r\n",
        "import time \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "\r\n",
        "# To download the prequisites, run this once. \r\n",
        "def nltk_downloader():\r\n",
        "  import nltk\r\n",
        "  nltk.download('punkt')\r\n",
        "  nltk.download('wordnet')\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    # Create the lemmatizer\r\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "    \r\n",
        "    # Get rid of non alpha characters except \"'\" as it is needed for the lemment\r\n",
        "    text = \"\".join(c for c in text if c.isalnum() or c == \" \" or \"'\")\r\n",
        "    \r\n",
        "    # Get rid of capitals\r\n",
        "    text = text.lower()\r\n",
        "    \r\n",
        "    # Tokenize the words    \r\n",
        "    # Create tokens of each word\r\n",
        "    token_text = word_tokenize(text)\r\n",
        "    \r\n",
        "    # Get rid of any piece of text that isn't over 2 characters\r\n",
        "    token_text = [t for t in token_text if len(t) > 2] \r\n",
        "    \r\n",
        "    # Put words in base form by doing lemmatization\r\n",
        "    token_text = [wordnet_lemmatizer.lemmatize(t) for t in token_text]\r\n",
        "    \r\n",
        "    # Return the tokens\r\n",
        "    return token_text\r\n",
        "\r\n",
        "\r\n",
        "# This function will get the term frequencies for word in the review\r\n",
        "# TF = Term I frequency in document/total words in document\r\n",
        "def calc_tf(term_count, review_corpus):\r\n",
        "    # A dictionary of all the term frequencies found\r\n",
        "    tf_freq = dict.fromkeys(term_count.keys(), 0)   \r\n",
        "    \r\n",
        "    # Review corpus is a tokenized list so the total words iteh length\r\n",
        "    total_words = len(review_corpus)\r\n",
        "    \r\n",
        "    # Calculate the term frequency for each word\r\n",
        "    for word, count in term_count.items():\r\n",
        "        tf_freq[word] = count/total_words\r\n",
        "        \r\n",
        "    return tf_freq\r\n",
        "\r\n",
        "\r\n",
        "# This calcualtes the idf\r\n",
        "# IDF = log(2)*(Total number of Documents/documents frequency or documents with term)\r\n",
        "def calc_idf(unique_terms, list_doc_terms):   \r\n",
        "    # A dicitonary of all the inverse document frequencies\r\n",
        "    idf = dict.fromkeys(unique_terms, 0)\r\n",
        "    \r\n",
        "    # Basically list_doc_terms has all the documents with the term count for each word\r\n",
        "    # You go through each document count the terms where they occured\r\n",
        "    for doc_terms in list_doc_terms:  \r\n",
        "        # This for loop is counting the amount of document a word was in\r\n",
        "        for word, value in doc_terms.items():\r\n",
        "            if 0 < value:\r\n",
        "                idf[word] += 1\r\n",
        "        \r\n",
        "    # Now we calculate idf\r\n",
        "    for word, value in idf.items():\r\n",
        "        idf[word] = math.log10(10 / float(value))\r\n",
        "    \r\n",
        "    return idf\r\n",
        "\r\n",
        "# Modified this function to return a list as dictionaries arn't needed anymore\r\n",
        "def calc_tf_idf(tf, idf, n_terms):\r\n",
        "    # Create an array that is of length of the number of unique terms\r\n",
        "    tf_idf_array = np.zeros(n_terms)\r\n",
        "    \r\n",
        "    for index, (word, value) in enumerate(tf.items()):\r\n",
        "        # Add the tfidf to the array\r\n",
        "        tf_idf_array[index] = value*idf[word]\r\n",
        "    \r\n",
        "    return tf_idf_array\r\n",
        "\r\n",
        "\r\n",
        "def process_text(text_data):\r\n",
        "     # A list of all the cleaned reviews\r\n",
        "    doc_list = []\r\n",
        "    \r\n",
        "    # List of all the unique terms\r\n",
        "    unique_terms = []\r\n",
        "    \r\n",
        "    # A list of all the term frequencies\r\n",
        "    tf_list = []\r\n",
        "    \r\n",
        "    for review in text_data:\r\n",
        "        # First clean the review\r\n",
        "        clean_review = clean_text(review)\r\n",
        "        \r\n",
        "        # Keeps track of the term counts for each word\r\n",
        "        count_dict = {}\r\n",
        "        \r\n",
        "        # Now lets find the total count for each word\r\n",
        "        for token in clean_review:\r\n",
        "            if token not in count_dict:\r\n",
        "                count_dict[token] = 1\r\n",
        "            else:\r\n",
        "                count_dict[token] += 1\r\n",
        "        \r\n",
        "        # Caclulate the term frequencies for each document\r\n",
        "        tf_list.append(calc_tf(count_dict, clean_review))\r\n",
        "        \r\n",
        "        # Then add the dictionary of counts for each document to the list\r\n",
        "        doc_list.append(count_dict)\r\n",
        "        \r\n",
        "        # Then add the new unique terms\r\n",
        "        unique_terms = set(unique_terms).union(set(clean_review))\r\n",
        "    \r\n",
        "    # Calculate the inverse document frequency value\r\n",
        "    idf = calc_idf(unique_terms, doc_list)\r\n",
        "    \r\n",
        "    # This array will contain the tfidf values for each term in each review\r\n",
        "    tfidf_values = np.zeros((len(tf_list), len(unique_terms)))\r\n",
        "    \r\n",
        "    # Now we can get the TFIDF for each document\r\n",
        "    for index, term_freq in enumerate(tf_list):\r\n",
        "        # This will return an array of the tfidf values calculated.\r\n",
        "        # The length of the unique terms list is passed in so that the \r\n",
        "        # Array that is returned matches the tfidf array\r\n",
        "        tf_idf_array = calc_tf_idf(term_freq, idf, len(unique_terms))\r\n",
        "        # Add this to the overall tfidf values calculated\r\n",
        "        tfidf_values[index,:] = tf_idf_array\r\n",
        "    \r\n",
        "    return tfidf_values\r\n",
        "    \r\n",
        "\r\n",
        "# Testing nltk on the dataset\r\n",
        "def dataset_testing():\r\n",
        "    print(\"=\"*50)\r\n",
        "\r\n",
        "    # Load the dataset\r\n",
        "    # imdb_df = pd.read_csv(\"IMDB Dataset.csv\")\r\n",
        "    imdb_df = pd.read_csv(\"/content/drive/MyDrive/CIT/FYP/ImplementationFiles/IMDB_Dataset.csv\")\r\n",
        "    print(\"Dataset loaded\")\r\n",
        "    print(\"=\"*50)\r\n",
        "\r\n",
        "    \r\n",
        "    # Change each positive and negative value to 1 and 0 respectively    \r\n",
        "    imdb_df['sentiment'] = imdb_df['sentiment'].map({'negative' : 0, 'positive' : 1})\r\n",
        "    \r\n",
        "    # For testing, a much smaller dataset is going to be used\r\n",
        "    imdb_df = imdb_df.head(5000)\r\n",
        "\r\n",
        "    # Group all the negative reviews and get the first 2500\r\n",
        "    imdb_df_neg = (imdb_df[imdb_df['sentiment'] == 0])[0:2500]\r\n",
        "    # Group all the positive and get the first 2500\r\n",
        "    imdb_df_pos = imdb_df[imdb_df['sentiment'] == 1]\r\n",
        "    \r\n",
        "    test_df = pd.concat([imdb_df_neg, imdb_df_pos]) \r\n",
        "    # print(test_df)\r\n",
        "    \r\n",
        "    # .values on a column of a dataframe returns a numpy array\r\n",
        "    # This is a numpy array of all the reviews\r\n",
        "    # initial_reviews = imdb_df['review'].values\r\n",
        "    initial_reviews = test_df['review'].values\r\n",
        "    \r\n",
        "    # This is a numpy array of all the positive and negativelabels\r\n",
        "    # labels = imdb_df['sentiment'].values\r\n",
        "    labels = test_df['sentiment'].values\r\n",
        "    \r\n",
        "    print(\"Creating Feature Vector\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    start = time.time()\r\n",
        "    # Process the text data and create teh feature vector\r\n",
        "    feature_vector = process_text(initial_reviews)\r\n",
        "    end = time.time()\r\n",
        "    print(\"Feature Vector Created\")\r\n",
        "    print(f\"Execution time is {end - start} secs\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    \r\n",
        "    # Shuffle the labesl and feature vector using sklearn shuffle\r\n",
        "    feature_vector, labels = shuffle(feature_vector, labels)\r\n",
        "    \r\n",
        "    # Creating train and test data\r\n",
        "    # Inital split will be 80:20 just for testing\r\n",
        "    no_samples = 0.8\r\n",
        "    \r\n",
        "    # This gets the percentage of indexes from feature vector and uses those for training\r\n",
        "    X_train = feature_vector[0:int(no_samples*len(feature_vector))]\r\n",
        "    y_train = labels[0:int(no_samples*len(labels))]\r\n",
        "    \r\n",
        "    # Go from the index that was used for training to the final\r\n",
        "    X_test = feature_vector[int(no_samples*len(feature_vector)):len(feature_vector)]\r\n",
        "    y_test = labels[int(no_samples*len(labels)):len(labels)]\r\n",
        "    \r\n",
        "    # Run a Logistic regression model just for a quick test\r\n",
        "    # model = LogisticRegression()\r\n",
        "    \r\n",
        "    # XGB can be a better generic tester so it is being used instead.\r\n",
        "    model = XGBClassifier()\r\n",
        "\r\n",
        "    print(\"Creating Model\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    start = time.time()\r\n",
        "    try:\r\n",
        "      model.fit(X_train, y_train)\r\n",
        "    except Exception as e:\r\n",
        "      print(e)\r\n",
        "    end = time.time()\r\n",
        "    print(\"Model Created\")\r\n",
        "    print(f\"Execution time is {end - start} secs\")\r\n",
        "    print(\"=\"*50)\r\n",
        "    \r\n",
        "    # print(\"Train accuracy:\", model.score(X_train, y_train))\r\n",
        "    # print(\"Test accuracy:\", model.score(X_test, y_test))\r\n",
        "\r\n",
        "    # Test the model\r\n",
        "    y_pred = model.predict(X_test)\r\n",
        "    predictions = [round(value) for value in y_pred]\r\n",
        "\r\n",
        "    # evaluate predictions\r\n",
        "    accuracy = accuracy_score(y_test, predictions)\r\n",
        "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\r\n",
        "    print(\"=\"*50)\r\n",
        "\r\n",
        "    print(\"Model Parameters:\\n\", model)\r\n",
        "\r\n",
        "def main():\r\n",
        "    # Download prequisites\r\n",
        "    nltk_downloader()\r\n",
        "    \r\n",
        "    # Test information on dataset\r\n",
        "    dataset_testing()\r\n",
        "    \r\n",
        "\r\n",
        "# Run Main\r\n",
        "main()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "==================================================\n",
            "Dataset loaded\n",
            "==================================================\n",
            "Creating Feature Vector\n",
            "==================================================\n",
            "Feature Vector Created\n",
            "Execution time is 32.44350337982178 secs\n",
            "==================================================\n",
            "Creating Model\n",
            "==================================================\n",
            "Model Created\n",
            "Execution time is 354.90329456329346 secs\n",
            "==================================================\n",
            "Accuracy: 52.21%\n",
            "==================================================\n",
            "Model Parameters:\n",
            " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
            "              nthread=None, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM-mXm-W48el"
      },
      "source": [
        "# Google Collab Test Area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apVlpCqBvidY"
      },
      "source": [
        "print(\"Hello World\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r75Fu2Iyvjph"
      },
      "source": [
        "from sklearn import datasets\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn import model_selection\r\n",
        "\r\n",
        "def example_mnist():\r\n",
        "  digits = datasets.load_digits()\r\n",
        "  targets = set(digits.target)\r\n",
        "\r\n",
        "  train_data = digits.data[0:int(0.8*len(digits.data))]\r\n",
        "  train_target = digits.target[0:int(0.8*len(digits.target))]\r\n",
        "  test_data = digits.data[int(0.8*len(digits.data)):len(digits.data)]\r\n",
        "  test_target = digits.target[int(0.8*len(digits.target)):len(digits.target)]\r\n",
        "\r\n",
        "  kf = model_selection.KFold(n_splits=2, shuffle=True)\r\n",
        "\r\n",
        "  best_score = 1e-10\r\n",
        "\r\n",
        "  for train_index,test_index in kf.split(train_data):\r\n",
        "      clf = linear_model.Perceptron()\r\n",
        "\r\n",
        "      clf.fit(train_data[train_index], train_target[train_index ])\r\n",
        "      prediction1 = clf.predict(train_data[test_index])\r\n",
        "\r\n",
        "      score = metrics.accuracy_score(train_target[test_index], prediction1)\r\n",
        "      print(\"Perceptron accuracy score: \", score)\r\n",
        "      print()\r\n",
        "\r\n",
        "      if score > best_score:\r\n",
        "          best_clf = clf\r\n",
        "\r\n",
        "\r\n",
        "  prediction = best_clf.predict(test_data)\r\n",
        "  for digit in targets:\r\n",
        "      print(digit, \" -> \", sum(test_target[prediction!=test_target]==digit))\r\n",
        "  \r\n",
        "  \r\n",
        "def main():\r\n",
        "  print(\"=\"*50)\r\n",
        "  example_mnist()\r\n",
        "\r\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}